{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/intro-logo-scala-spa.png\" align=\"left\" width=\"600px\"/>\n",
    "<!--- ![alt text](heading.png \"Heading with Scala logo\") --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "\n",
    "### [4. Ya sólo nos queda... ¡volar!](#seccion-4. Ya sólo nos queda... ¡volar!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"seccion-4. Ya sólo nos queda... ¡volar!\"></a>\n",
    "<table align=\"left\" style=\"border-collapse: collapse; width: 100%; border: 5px double black\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 100px;\">\n",
    "<img src=\"icons/volar-m.png\" align=\"left\" width=85px/>\n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left;\">\n",
    "<h1>4. Ya sólo nos queda... ¡volar!</h1>\n",
    "<br>\n",
    "Después de conocer la sintaxis y las características más importantes de Scala, solo nos queda poner en práctica todo lo aprendido para llevar a cabo tareas más avanzadas e interesantes.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "<a name=\"subseccion-Introduccion a Spark\"></a>\n",
    "## Introducción a Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "En este notebook presentaremos los conceptos básicos de Spark con Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "#### Hasta ahora:\n",
    "Las tareas de \"data science\" y análisis de datos se ha llevado a cabo “a pequeña escala”, en R/Python/MATLAB, etc.\n",
    "\n",
    "\n",
    "#### Hoy en día:\n",
    "Los conjuntos de datos ya no caben en memoria, así que...\n",
    "\n",
    "* Estos lenguajes/frameworks no nos permiten escalar. \n",
    "\n",
    "* Hay que reimplementarlo todo en algún otro lenguaje o sistema.\n",
    "\n",
    "\n",
    "#### Además:\n",
    "\n",
    "* La industria se está moviendo hacia una Inteligencia Empresarial basada en la toma de decisiones orientada a los datos, que se apoya en enormes conjuntos de datos.\n",
    "\n",
    "* La API de Spark guarda una relación casi de 1-a-1 con las colecciones de Scala, pero ¡distribuidas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "### Spark + Scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "* Más expresivo. APIs modeladas tras las colecciones de Scala. ¡Parecen listas funcionales! \n",
    "\n",
    "\n",
    "* Más rico, con más operaciones componibles posibles que en MapReduce (Hadoop).\n",
    "\n",
    "\n",
    "* Eficiente: en terminos de tiempo de ejecución... ¡Y también en términos de la productividad del desarrollador! \n",
    "\n",
    "\n",
    "* Bueno para 'data science'. No solo por el rendimiento, sino porque permite iteraciones (eficientes), algo requerido por la mayoría de los algoritmos presentes en la caja de herramientas de un 'data scientist'.\n",
    "\n",
    "\n",
    "* Alta demanda de desarrolladores de Spark y Scala ¡y de 'data scientists'!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "### Spark vs Hadoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "\n",
    "\n",
    "* Hadoop es una implementación 'open source' del MapReduce de Google.\n",
    "\n",
    "\n",
    "* Una API simple para operaciones map y reduce sobre conjuntos de datos distribuidos.\n",
    "\n",
    "\n",
    "* Tolerancia a fallos: entre cada operación map y reduce, escribe datos intermedios para ser capaz de recuperarse de fallos.\n",
    "\n",
    "\n",
    "* La tolerancia a fallos de Spark es mucho más eficiente porque:\n",
    "    - Mantiene todos los datos inmutables y en memoria\n",
    "    - Las operaciones son transformaciones funcionales\n",
    "    - Tolerancia a fallos: volver a aplicar las transformaciones a los datos originales\n",
    "\n",
    "\n",
    "* Spark es compatible con HDFS (Hadoop Distributed FileSystem)\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "header"
    ]
   },
   "source": [
    "<a name=\"subseccion-Conceptos principales en Spark\"></a>\n",
    "## Conceptos principales en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "text"
    ]
   },
   "source": [
    "* Spark Session: una conexión a la API de Spark\n",
    "\n",
    "\n",
    "* Estructura Hardware:\n",
    "    - Cluster de master + workers\n",
    "    - Workflow: shuffling\n",
    "\n",
    "\n",
    "* Estructuras de datos lógicas:\n",
    "    - RDDs\n",
    "    - PairRDDs\n",
    "    \n",
    "    \n",
    "* Operaciones básicas:\n",
    "    - Transformaciones\n",
    "    - Acciones\n",
    "    \n",
    "    \n",
    "* Librerías interesantes:\n",
    "    - Spark SQL: DataFrames y Datasets\n",
    "    - Spark Streaming API\n",
    "    - MlLib\n",
    "    - GraphX\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "### Spark Session\n",
    "\n",
    "Conexión al cluster de Spark. \n",
    "\n",
    "Normalmente le \"hablaremos\" al nodo `master` del cluster, y este le envia las tareas a los nodos `worker`.\n",
    "\n",
    "`SparkSession` es el objeto que usaremos para llevar a cabo las operaciones de configuración y entrada contra el cluster.\n",
    "\n",
    "#### Configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:17: error: not found: value SparkSession\n",
       "       val spark = SparkSession\n",
       "                   ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Spark basic example\")     // Nombre para la sesión\n",
    "      .master(\"local[2]\")                 // Ruta y número de cores a utilizar\n",
    "      .getOrCreate()\n",
    "\n",
    "// Opcional: Ajustar el nivel de logging si log4j no está configurado\n",
    "Logger.getRootLogger.setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Las versiones anteriores de Spark usan <strong>SparkContext</strong> en lugar de <strong>SparkSession</strong></li>\n",
    "                <li><strong>SparkContext</strong> todavía se usa, pero es transparente al desarrollador</li>\n",
    "                <li>Se puede acceder a <strong>SparkContext</strong> a través de <strong>SparkSession</strong>: <strong>spark.sparkContext</strong></li>\n",
    "                <li>En este notebook, ya tenemos a ambos, un <strong>SparkSession</strong> y su correspondiente  <strong>SparkContext</strong> en las variables inmutables: <strong>spark</strong> y <strong>sc</strong>, respectivamente, las cuales se usarán a lo largo del notebook</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Hello World!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 100).reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Usamos <strong>sc</strong> para construir una colección paralela en un cluster de Spark.</li>\n",
    "                <li><strong>parallelize</strong> es una función para transformar una colección a su correspondiente versión paralela.</li>\n",
    "                <li><strong>(1 to 100)</strong> es la definición de una colección de rango (una colección formada por los valores en el rango dado)</li>\n",
    "                <li><strong>reduce</strong> tiene el mismo significado que en la API Collection de Scala</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitudes entre Spark y Scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "La API de Spark tiene casi una relación 1-a-1 con la API de colecciones de Scala. Veamos un ejemplo:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María => 5\n",
      "Pedro => 5\n",
      "Elisa => 5\n",
      "Juan => 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lista: List[String] = List(Juan, María, Pedro, Elisa)\n",
       "paresLista: List[(String, Int)] = List((Juan,4), (María,5), (Pedro,5), (Elisa,5))\n",
       "res1: Int = 19\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lista = List(\"Juan\", \"María\", \"Pedro\", \"Elisa\")                   // Construimos una List[String] de Scala \n",
    "\n",
    "val paresLista = lista.map(nombre => (nombre, nombre.length))         // Associar la longitud de cada string\n",
    "\n",
    "paresLista.sortBy(-_._2).foreach(t => println(t._1 + \" => \" + t._2))  // Lo imprimimos\n",
    "\n",
    "paresLista.map(_._2).reduce(_ + _)                                    // Sumar las longitudes de los strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "El equivalente en Spark, de manera distribuida, podría ser así:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María => 5\n",
      "Pedro => 5\n",
      "Elisa => 5\n",
      "Juan => 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parlista: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27\n",
       "pares: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:29\n",
       "res2: Int = 19\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parlista = sc.parallelize[String](lista)                      // Crear el equivalente: ParallelCollectionRDD[String]\n",
    "\n",
    "val pares = parlista.map(nombre => (nombre, nombre.length))       // pares: MapPartitionRDD[(String, Int)]\n",
    "\n",
    "pares.sortBy(-_._2).collect.foreach(t => println(t._1 + \" => \" + t._2))\n",
    "\n",
    "pares.map(_._2).reduce(_ + _)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Correspondencia 1-a-1 entre Scala y Spark</li>\n",
    "                <li><strong>collect</strong> recopila en el nodo 'master' los bloques de datos distribuidos por los nodos 'worker', para así poder procesar todo el RDD</li>\n",
    "                <li>Spark usa evaluación 'lazy' para algunas funciones (map, sortBy) y evaluación 'eager' para otras (reduce, collect). Lo estudiaremos con más detalle a continuación.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subseccion-Estructura hardware en Spark\"></a>\n",
    "## Estructura hardware en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/spark_structure.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Workflow\n",
    "\n",
    "* El nodo 'master' distribuye los datos en bloques sobre los nodos 'worker', envía las tareas e integra los resultados de la computación.\n",
    "\n",
    "\n",
    "* Los nodos 'worker' reciben los fragmentos de datos y las tareas y llevan a cabo las transformaciones y acciones sobre sus bloques de datos.\n",
    "\n",
    "\n",
    "* Cada vez que nuestro proceso requiere todo el conjunto de datos para llevar a cabo una acción, el nodo 'master' recupera los bloques de datos de los 'workers', y reconstruye los datos en memoria.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Cuando los datos viajan a través de la red, se denomina <em>shuffle</em> y es <strong>realmente costoso</strong></li>\n",
    "                <li>Debemos minimizar el número de veces que se requiere un <em>shuffle</em> en nuestra aplicación.</li>\n",
    "                <li>Pero vamos a tomarlo con calma por ahora: necesitamos saber más conceptos relacionados con Spark para estudiar con mayor profundidad este tema.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subseccion-Estructuras de datos lógicas RDDs\"></a>\n",
    "## Estructuras de datos lógicas: RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Resilient Distributed Dataset\n",
    "\n",
    "\n",
    "* Colecciones paralelas para la computación distribuida de programación funcional\n",
    "\n",
    "\n",
    "* Una colección de datos (tipados), que se puede distribuir fácilmente sobre los nodos 'worker', de manera que cada nodo se hace cargo de un trozo de todo el conjunto de datos a procesar.\n",
    "\n",
    "\n",
    "* Un RDD es una referencia lógica a un conjunto de datos que es fragmentado a través de muchos servidores en el cluster de Spark.\n",
    "\n",
    "\n",
    "* Los RDD son fragmentados y distribuidos sobre los nodos 'worker' en el cluster de Spark de forma automática (sin la intervención del programador). Ver la sección anterior sobre la estructura física de un cluster de Spark.\n",
    "\n",
    "\n",
    "* El esquema de fragmentación puede modificarse, pero por defecto Spark intenta minimizar el tráfico de la red entre los nodos cuando procesa los RDD. Por ejemplo: en un entorno local, hay normalmente una partición por cada nodo 'worker' (los cores de la CPU disponibles para Spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Ejemplo: Leyendo de un fichero Json a un RDD\n",
    "\n",
    "\n",
    "Fichero Json de entrada:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[...]\n",
    "\n",
    "{\n",
    "    \"idTweet\":\"915831976929714177\",\n",
    "    \n",
    "    \"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\n",
    "    \n",
    "    \"date\":\"Thu Oct 05 08:52:13 CEST 2017\",\n",
    "    \n",
    "    \"authorId\":\"2885455811\",\n",
    "    \n",
    "    \"idOriginal\":\"915523419281739776\"\n",
    "}\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instrucciones en Spark para leer el fichero y construir el RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the file: 10\n",
      "Number of partitions in the RDD: 2\n"
     ]
    }
   ],
   "source": [
    "val testRDD = sc.textFile(\"test.txt\")            // Lee del fichero a un RDD\n",
    "val nRecords = testRDD.count                     // Devuelve el número de registros leidos del fichero Json\n",
    "val nPartitions = testRDD.partitions.size        // Devuelve el número de particiones de testRDD\n",
    "\n",
    "println(\"Number of records in the file: \" + nRecords)\n",
    "println(\"Number of partitions in the RDD: \" + nPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vamos a jugar un poco con nuestro nuevo RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Imprime los primeros 5 elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row: {\"idTweet\":\"915831976929714177\",\"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\"date\":\"Thu Oct 05 08:52:13 CEST 2017\",\"authorId\":\"2885455811\",\"idOriginal\":\"915523419281739776\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915831940745441280\",\"text\":\"Yo ya he escogido mediador. https://t.co/D7xS4MHbDG\",\"date\":\"Thu Oct 05 08:52:04 CEST 2017\",\"authorId\":\"2099361\",\"idOriginal\":\"\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915831968301973504\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:11 CEST 2017\",\"authorId\":\"799792832\",\"idOriginal\":\"915830958443687936\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915831985582612480\",\"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\"date\":\"Thu Oct 05 08:52:15 CEST 2017\",\"authorId\":\"105157939\",\"idOriginal\":\"915523419281739776\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915832004658286593\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:19 CEST 2017\",\"authorId\":\"124248712\",\"idOriginal\":\"915830958443687936\"}\n"
     ]
    }
   ],
   "source": [
    "testRDD.take(5).foreach(elem => println(\"\\nRow: \" + elem ))    // Toma 5 elementos del RDD y los imprime en la consola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Esto es solo parte del fichero de entrada que se usará en los próximos ejemplos</li>\n",
    "                <li>Con esta instrucción leemos un Json como un fichero de texto plano</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Filtra aquellos tuits que tengan cualquier hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: {\"idTweet\":\"915831968301973504\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:11 CEST 2017\",\"authorId\":\"799792832\",\"idOriginal\":\"915830958443687936\"}\n",
      "Row: {\"idTweet\":\"915832004658286593\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:19 CEST 2017\",\"authorId\":\"124248712\",\"idOriginal\":\"915830958443687936\"}\n",
      "Row: {\"idTweet\":\"915830958443687936\",\"text\":\"#AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:48:10 CEST 2017\",\"authorId\":\"110117638\",\"idOriginal\":\"\"}\n",
      "Row: {\"idTweet\":\"915832008936509440\",\"text\":\"RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr\",\"date\":\"Thu Oct 05 08:52:20 CEST 2017\",\"authorId\":\"150587014\",\"idOriginal\":\"915830945785237504\"}\n",
      "Row: {\"idTweet\":\"915832057288433664\",\"text\":\"RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… \",\"date\":\"Thu Oct 05 08:52:32 CEST 2017\",\"authorId\":\"273360453\",\"idOriginal\":\"915808416639143936\"}\n",
      "Row: {\"idTweet\":\"915808416639143936\",\"text\":\"Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz\",\"date\":\"Thu Oct 05 07:18:36 CEST 2017\",\"authorId\":\"184865048\",\"idOriginal\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "val hashtagTweets = testRDD.filter(t => t.contains(\"#\"))\n",
    "hashtagTweets.collect.foreach(elem => println(\"Row: \" + elem ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Extrae los ids de los tuits y sus autores y el tuit original (si es un retuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"idTweet\":\"915831976929714177, authorId\":\"2885455811, idOriginal\":\"915523419281739776\"}]\n",
      "[{\"idTweet\":\"915831940745441280, authorId\":\"2099361, idOriginal\":\"\"}]\n",
      "[{\"idTweet\":\"915831968301973504, authorId\":\"799792832, idOriginal\":\"915830958443687936\"}]\n",
      "[{\"idTweet\":\"915831985582612480, authorId\":\"105157939, idOriginal\":\"915523419281739776\"}]\n",
      "[{\"idTweet\":\"915832004658286593, authorId\":\"124248712, idOriginal\":\"915830958443687936\"}]\n",
      "[{\"idTweet\":\"915830958443687936, authorId\":\"110117638, idOriginal\":\"\"}]\n",
      "[{\"idTweet\":\"915832008936509440, authorId\":\"150587014, idOriginal\":\"915830945785237504\"}]\n",
      "[{\"idTweet\":\"915832057288433664, authorId\":\"273360453, idOriginal\":\"915808416639143936\"}]\n",
      "[{\"idTweet\":\"915808416639143936, authorId\":\"184865048, idOriginal\":\"\"}]\n",
      "[{\"idTweet\":\"915836526789046273, authorId\":\"142775869, idOriginal\":\"\"}]\n"
     ]
    }
   ],
   "source": [
    "val ids = testRDD.map(t => t.split(\"\\\",\\\"\")).map(fields => (fields(0), fields(3), fields(4)))\n",
    "\n",
    "ids.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Limpia los elementos del RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[915831976929714177, 2885455811, 915523419281739776]\n",
      "[915831940745441280, 2099361, ]\n",
      "[915831968301973504, 799792832, 915830958443687936]\n",
      "[915831985582612480, 105157939, 915523419281739776]\n",
      "[915832004658286593, 124248712, 915830958443687936]\n",
      "[915830958443687936, 110117638, ]\n",
      "[915832008936509440, 150587014, 915830945785237504]\n",
      "[915832057288433664, 273360453, 915808416639143936]\n",
      "[915808416639143936, 184865048, ]\n",
      "[915836526789046273, 142775869, ]\n"
     ]
    }
   ],
   "source": [
    "val cleanIds = ids.map(tuple => {\n",
    "    (tuple._1.replace(\"{\\\"idTweet\\\":\\\"\", \"\"), \n",
    "    tuple._2.replace(\"authorId\\\":\\\"\",\"\"), \n",
    "    tuple._3.replace(\"idOriginal\\\":\\\"\",\"\").replace(\"\\\"}\",\"\"))\n",
    "    })\n",
    "\n",
    "cleanIds.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Convierte el RDD[(String, String, String)] en un RDD[(Long, Long, Long)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 10.0 failed 1 times, most recent failure: Lost task 1.0 in stage 10.0 (TID 20, localhost, executor driver): java.lang.NumberFormatException: For input string: \"\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Long.parseLong(Long.java:601)\n",
       "\tat java.lang.Long.parseLong(Long.java:631)\n",
       "\tat scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)\n",
       "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:29)\n",
       "\tat $line42.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:28)\n",
       "\tat $line42.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:25)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Long.parseLong(Long.java:601)\n",
       "\tat java.lang.Long.parseLong(Long.java:631)\n",
       "\tat scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)\n",
       "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:29)\n",
       "\tat $anonfun$1.apply(<console>:28)\n",
       "\tat $anonfun$1.apply(<console>:25)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
       "  ... 40 elided\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"\"\n",
       "  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "  at java.lang.Long.parseLong(Long.java:601)\n",
       "  at java.lang.Long.parseLong(Long.java:631)\n",
       "  at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)\n",
       "  at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)\n",
       "  at $anonfun$1.apply(<console>:28)\n",
       "  at $anonfun$1.apply(<console>:25)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "  at scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val longIds = cleanIds.map(tuple => {\n",
    "    (tuple._1.toLong, \n",
    "    tuple._2.toLong, \n",
    "    tuple._3.toLong)\n",
    "    })\n",
    "\n",
    "longIds.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ¿Lo volvemos a intentar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[915831976929714177, 2885455811, 915523419281739776]\n",
      "[915831940745441280, 2099361, 0]\n",
      "[915831968301973504, 799792832, 915830958443687936]\n",
      "[915831985582612480, 105157939, 915523419281739776]\n",
      "[915832004658286593, 124248712, 915830958443687936]\n",
      "[915830958443687936, 110117638, 0]\n",
      "[915832008936509440, 150587014, 915830945785237504]\n",
      "[915832057288433664, 273360453, 915808416639143936]\n",
      "[915808416639143936, 184865048, 0]\n",
      "[915836526789046273, 142775869, 0]\n"
     ]
    }
   ],
   "source": [
    "val longIds = cleanIds.map(tuple => {\n",
    "    (tuple._1.toLong, \n",
    "    tuple._2.toLong, \n",
    "    if(tuple._3 == \"\") 0 else tuple._3.toLong)\n",
    "    })\n",
    "\n",
    "longIds.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Recupera los tuits que han sido retuiteados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915830958443687936,110117638,0)\n",
      "(915808416639143936,184865048,0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(915523419281739776, 915830945785237504, 915830958443687936, 915808416639143936)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retweetedIDS = longIds.filter(t => t._3!=0).map(_._3).distinct.collect\n",
    "\n",
    "val original = longIds.filter(t => retweetedIDS.contains(t._1))\n",
    "\n",
    "\n",
    "original.collect.foreach(println _)\n",
    "retweetedIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subseccion-Estructuras de datos lógicas PairRDDs\"></a>\n",
    "## Estructuras de datos lógicas: PairRDDs\n",
    "\n",
    "\n",
    "* Intuición: versión paralela y distribuida de un Map\n",
    "\n",
    "\n",
    "* Un RDD que contiene tuplas de (clave, valor)\n",
    "\n",
    "\n",
    "* Muy útil porque los Map son una de las abstracciones de datos más utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso de uso de PairRDDs: Contando palabras en un RDD\n",
    "\n",
    "\n",
    "1.- Primero: vamos a dividir el contenido del RDD en palabras: usando <strong>flatMap</strong>\n",
    "\n",
    "2.- Después, crea un PairRDD con: (Palabra, 1): usando <strong>map</strong>\n",
    "\n",
    "3.- Finalmente, agrupa cada par en función de su primer componente (la palabra) y suma los segundos componentes (ocurrencias de las palabras): usando la función <strong>reduceByKey</strong> de los PairRDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas leídas: 101\n",
      "Word: #ranciofacts\tOccurrences: 3\n",
      "Word: recodo\tOccurrences: 1\n",
      "Word: 2017\",\"authorId\":\"124248712\",\"idOriginal\":\"915830958443687936\"}\tOccurrences: 1\n",
      "Word: arreglan\tOccurrences: 2\n",
      "Word: https://t.co/trDDTvrjgr\",\"date\":\"Thu\tOccurrences: 1\n",
      "Word: Si\tOccurrences: 1\n",
      "Word: donde\tOccurrences: 1\n",
      "Word: ya\tOccurrences: 1\n",
      "Word: los\tOccurrences: 4\n",
      "Word: {\"idTweet\":\"915830958443687936\",\"text\":\"#AmicsAmigos\tOccurrences: 1\n",
      "Word: 2017\",\"authorId\":\"142775869\",\"idOriginal\":\"\"}\tOccurrences: 1\n",
      "Word: x\tOccurrences: 1\n",
      "Word: @pedroveraOyP:\tOccurrences: 2\n",
      "Word: #AmicsAmigos\tOccurrences: 2\n",
      "Word: pero...\tOccurrences: 2\n"
     ]
    }
   ],
   "source": [
    "// Mecanismo habitual para contar elementos mapeando un RDD a un PairRDD\n",
    "val countWords = testRDD.flatMap(line => line.split(\" \")).map(w => (w, 1)).reduceByKey(_ + _)\n",
    "\n",
    "println(\"Filas leídas: \" + countWords.count)\n",
    "\n",
    "// Imprimiendo\n",
    "countWords.take(15).foreach(t => println(\"Word: \" + t._1 + \"\\tOccurrences: \" + t._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a pensarlo dos veces...\n",
    "\n",
    "\n",
    "* El primer paso va desde un RDD[String] a un RDD[String]: flatMap divide cada <em>Tweet</em> en una Collection[<em>palabras</em>], y luego las aplana, obteniendo un RDD[<em>palabras</em>].\n",
    "\n",
    "\n",
    "* El segundo paso va de un RDD[String], donde cada String es una palabra, a un RDD[(String, Int)], que es un PairRDD[(String, Int)].\n",
    "\n",
    "* Finalmente, <strong>reduceByKey</strong> agrupa todas las tuplas con la misma palabra, sumando sus valores, produciendo un PairRDD[(String, Int)] que representa un RDD[<em>(palabra, ocurrenciasDePalabra)</em>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>En el ejemplo de arriba, cogemos cada linea del fichero Json como un String.</li>\n",
    "                <li>Eso significa que estamos contando todas las palabras en los textos, incluyendo los nombres de los campos Json, y cosas que no son solo texto.</li>\n",
    "                <li>¿Cómo podemos arreglarlo? Vamos a verlo en las siguientes secciones.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué estamos haciendo realmente?\n",
    "\n",
    "\n",
    "Cuando leemos un fichero con <strong>textFile</strong>, Spark crea un RDD[String]. No infiere la estructura del Json, ni de los pares (campo, valor).\n",
    "\n",
    "\n",
    "La primera idea para solucionarlo podría ser algo como lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val texts = testRDD.map(row => row.split(\"\\\",\")).map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué hace la instrucción de arriba con el RDD[String] original?\n",
    "\n",
    "Vamos a imprimirlo y lo vemos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "Yo ya he escogido mediador. https://t.co/D7xS4MHbDG\n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n"
     ]
    }
   ],
   "source": [
    "texts.take(5).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora, podemos contar las palabras de cada texto siguiendo el mismo procedimiento que antes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val countWordsTexts = texts.flatMap(line => line.split(\" \")).map(w => (w, 1)).reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \tOccurrences: 8\n",
      "Word: a\tOccurrences: 8\n",
      "Word: no\tOccurrences: 7\n",
      "Word: RT\tOccurrences: 6\n",
      "Word: que\tOccurrences: 5\n",
      "Word: los\tOccurrences: 4\n",
      "Word: tenemos\tOccurrences: 4\n",
      "Word: lo\tOccurrences: 4\n",
      "Word: todos\tOccurrences: 4\n",
      "Word: #ranciofacts\tOccurrences: 3\n",
      "Word: https://t.co/mjMhHQfHuB\tOccurrences: 3\n",
      "Word: #AmicsAmigos\tOccurrences: 3\n",
      "Word: es\tOccurrences: 3\n",
      "Word: pelearsen\tOccurrences: 3\n",
      "Word: muy\tOccurrences: 3\n"
     ]
    }
   ],
   "source": [
    "// Printing it out, sorted by the counting\n",
    "countWordsTexts.sortBy(-_._2).take(15).foreach(t => println(\"Word: \" + t._1 + \"\\tOccurrences: \" + t._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pros de los RDD y los PairRDD\n",
    "\n",
    "\n",
    "    1.- Fácil usar la API, basados en las colecciones de Scala (map, reduce, filter, flatMap...)\n",
    "    \n",
    "    2.- Optimizados para ser usados en un cluster distribuido de Spark\n",
    "    \n",
    "    3.- Colecciones tipadas: apoyándose en la inferencia de tipos de Scala\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Pero, realmente... es un poco tedioso, ¿no es cierto?\n",
    "\n",
    "    1.- No son buenos para procesar datos estructurados o semi-estructurados: \n",
    "        - En el ejemplo, intentamos leer un fichero <strong>estructurado</strong>, en formato Json\n",
    "        - Así que perdimos toda esa <strong>información preciada</strong> (campos, valores, etc.) transformándolo en una colección (resiliente y distribuible) de strings.\n",
    "        - Y luego usamos los mismos <strong>split-get-replace</strong> viejos y aburridos de la clase String para extraer las partes interesantes del string.\n",
    "\n",
    "\n",
    "    2.- El shuffling puede convertirse en el cuello de botella de nuestra aplicación y a veces no es fácil de evitar.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>En relación al <strong>data shuffle</strong>, en la siguiente sección estudiaremos las operaciones básicas en Spark (<em>Transformaciones</em> y <em>Acciones</em>), sus efectos y la manera en que se gestionan en el cluster de Spark.</li>\n",
    "                <li>Respecto al procesado de <strong>información estructurada y semi-estructurada</strong>: Spark ofrece una manera mucho mejor de lidiar con este tipo de datos a través de la librería <em>Spark SQL</em> y sus estructuras de datos relacionales: <em>DataFrames</em> y <em>Datasets</em>. Las estudiaremos más adelante.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subseccion-Operaciones Básicas de Spark: Transformaciones y Acciones\"></a>\n",
    "## Operaciones Básicas de Spark: Transformaciones y Acciones\n",
    "\n",
    "\n",
    "Los RDD de Apache Spark soportan dos tipos de operaciones: Transformaciones y Acciones.\n",
    "\n",
    "\n",
    "\n",
    "### Transformaciones\n",
    "\n",
    "\n",
    "* Son funciones que producen nuevos RDD a partir de los ya existentes. Ejemplos: map(), filter().\n",
    "\n",
    "\n",
    "* Dado que los RDD de entrada no pueden modificarse (son inmutables por naturaleza), cada vez que aplicamos una transformación se crean nuevos RDD.\n",
    "\n",
    "\n",
    "* Las transformaciones se evalúan con evaluación \"perezosa\", lo que significa que no se ejecutan de inmediato. Una transformación se ejecuta efectivamente cuando llamamos a una acción.\n",
    "\n",
    "\n",
    "* Por lo tanto, aplicar una (cantidad de) transformaciones no produce ningún efecto inmediato. En cambio, se crea un linaje de RDD, que va del RDD original (que invoca la primera transformación) a los RDD finales (resultado de todas las transformaciones). El linaje de RDD, representado por un <strong>DAG</strong> (Directed Acyclic Graph o Grafo acíclico dirigido), es un plan de ejecución lógica de todas las transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos de transformaciones y del DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: String =\n",
       "(2) ShuffledRDD[6] at reduceByKey at <console>:33 []\n",
       " +-(2) MapPartitionsRDD[5] at map at <console>:33 []\n",
       "    |  MapPartitionsRDD[4] at flatMap at <console>:33 []\n",
       "    |  MapPartitionsRDD[3] at map at <console>:32 []\n",
       "    |  MapPartitionsRDD[2] at map at <console>:32 []\n",
       "    |  test.txt MapPartitionsRDD[1] at textFile at <console>:25 []\n",
       "    |  test.txt HadoopRDD[0] at textFile at <console>:25 []\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Recordar el código visto anteriormente: \n",
    "    // val texts = testRDD.map(row => row.split(\"\\\",\")).map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\"))\n",
    "    // val countWordsTexts = texts.flatMap(line => line.split(\" \")).map(w => (w, 1)).reduceByKey(_ + _)\n",
    "\n",
    "countWordsTexts.toDebugString            // imprimir el plan de ejecución (DAG) de las transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de transformaciones:\n",
    "\n",
    "\n",
    "\n",
    "* Transformaciones estrechas ('narrow'): no implican una mezcla de datos. Se pueden calcular por cada nodo 'worker' con sus propias particiones de datos.\n",
    "     - Ejemplos: map, filter, flatMap, union, sample...\n",
    "\n",
    "\n",
    "\n",
    "* Transformaciones amplias ('wide'): la lógica de procesamiento depende de los datos de múltiples particiones, por lo que es necesario combinar los datos para reunirlos en un solo lugar.\n",
    "     - Ejemplos: distinct, join, reduceByKey, groupByKey...\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Spark implementa un mecanismo para optimizar el plan de ejecución de las transformaciones con el fin de minimizar la combinación de datos ('data shuffling')</li>\n",
    "                <li>Recuerda que las transformaciones son <strong>lazy</strong>: no se ejecutan cuando se declaran</li>\n",
    "                <li>Una forma de realizar un conjunto de transformaciones es aplicar una acción al RDD de salida</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>DAG es el mecanismo que permite que Spark sea tolerante a fallos, <strong>sin</strong> tener que escribir datos en el disco como una copia de seguridad</li>\n",
    "                <li>Spark se recupera de las fallos volviendo a calcular las particiones perdidas, siguiendo el <strong>DAG</strong></li>\n",
    "                <li>Es realmente <strong>rápido</strong> recuperar datos de transformaciones <strong>narrow</strong>, pero <strong>lento</strong> hacerlo de transformaciones <strong>wide</strong></li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Acciones:\n",
    "\n",
    "\n",
    "\n",
    "* Son operaciones Spark sobre RDD que producen valores que no son RDD.\n",
    "\n",
    "\n",
    "* Los resultados de las acciones se almacenan en los nodos 'master' o en el sistema de almacenamiento externo. Por lo tanto, una acción es una de las maneras de enviar datos desde los nodos 'worker' al 'master'.\n",
    "\n",
    "\n",
    "* Pone el modo 'lazy' de los RDD en movimiento, lo que significa que una acción provoca la ejecución de las transformaciones asociadas en el RDD.\n",
    "\n",
    "* Ejemplos: count, collect, first, take..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repasemos el ejemplo de RDD desde un archivo de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originalRDD: org.apache.spark.rdd.RDD[String] = ./test.txt MapPartitionsRDD[8] at textFile at <console>:25\n",
       "firstTransformation: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[9] at map at <console>:27\n",
       "secondTransformation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at map at <console>:29\n",
       "thirdTransformation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at <console>:31\n",
       "fourthTransformation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at flatMap at <console>:33\n",
       "fifthTransformation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at filter at <console>:35\n",
       "sixthTransformation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at distinct at <console>:37\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val originalRDD = sc.textFile(\"./test.txt\")           // Leer fichero de texto plano\n",
    "\n",
    "val firstTransformation = originalRDD.map(row => row.split(\"\\\",\"))\n",
    "\n",
    "val secondTransformation = firstTransformation.map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\"))\n",
    "\n",
    "val thirdTransformation = secondTransformation.filter(text => text.contains(\"@\"))\n",
    "\n",
    "val fourthTransformation = secondTransformation.flatMap(text => text.split(\" \"))\n",
    "\n",
    "val fifthTransformation = fourthTransformation.filter(word => word.startsWith(\"#\"))\n",
    "\n",
    "val sixthTransformation = fifthTransformation.map(_.toLowerCase).distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/question.jpg\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>¿Qué hemos hecho, hasta el momento?</li>\n",
    "                <li>¿Cuál es el contenido de cada RDD?</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr\n",
      "RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… \n",
      "Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n",
      "#amicsamigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n"
     ]
    }
   ],
   "source": [
    "thirdTransformation.collect.foreach(println)         // Transformación a computar: 3, 2 y 1\n",
    "println\n",
    "fifthTransformation.collect.foreach(println)         // Transformación a computar: 5, 4, 2 y 1\n",
    "println\n",
    "sixthTransformation.collect.foreach(println)         // Transformación a computar: 6, 5, 4, 2 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Recuerda que las transformaciones se evaluan mediante evaluación perezosa, así que... </li>\n",
    "                <li>Fíjate en que las transformaciones 2 y 1 se evalúan ¡¡tres veces!!</li>\n",
    "                <li>Spark proporciona un mecanismo para ayudar a los programadores a evitar esta situación: <strong>caching</strong>. Vamos a reescribir nuestro código:</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val originalRDD2 = sc.textFile(\"test.txt\")           // leer el fichero de texto plano\n",
    "\n",
    "val firstT = originalRDD2.map(row => row.split(\"\\\",\"))\n",
    "\n",
    "val secondT = firstT.map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\")).cache    // ¡¡Guardar el resultado en la caché!!\n",
    "\n",
    "val thirdT = secondT.filter(text => text.contains(\"@\"))\n",
    "\n",
    "val fourthT = secondT.flatMap(text => text.split(\" \"))\n",
    "\n",
    "val fifthT = fourthT.filter(word => word.startsWith(\"#\"))\n",
    "\n",
    "val sixthT = fifthT.map(_.toLowerCase).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr\n",
      "RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… \n",
      "Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n",
      "#amicsamigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n"
     ]
    }
   ],
   "source": [
    "thirdT.collect.foreach(println)         // Transformación a computar: 3, 2 y 1, y guarda en la caché la transformación 2\n",
    "println\n",
    "fifthT.collect.foreach(println)         // Transformación a computar: 5 and 4 sobre la ya ya evaluada y guardada 2\n",
    "println\n",
    "sixthT.collect.foreach(println)         // Transformación a computar: 6, 5, 4 sobre la ya ya evaluada y guardada 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subseccion-Spark SQL\"></a>\n",
    "## Spark SQL: DataFrames and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Características de Spark SQL\n",
    "\n",
    "\n",
    "* Librería de Spark que integra la sintaxis basada en SQL para realizar operaciones en datos distribuidos.\n",
    "\n",
    "\n",
    "* Define estructuras de datos para facilitar la implementación de operaciones relacionales (select, group-by, order-by, max, min, average, count, etc.): DataFrames y Datasets.\n",
    "\n",
    "\n",
    "* Estas estructuras de datos integran optimizaciones de rendimiento del álgebra relacional de SQL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Para utilizar la sintaxis optimizada de Spark SQL debemos incluir la siguiente línea de código: <em>import spark.implicits._</em></li>\n",
    "                <li>También es útil para transformar los RDD en DataFrames</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// En el notebook de Jupyter esta línea deberá ser diferente\n",
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subsubseccion-DataFrames\"></a>\n",
    "### DataFrames\n",
    "\n",
    "* Conceptualmente equivalente a una tabla SQL\n",
    "\n",
    "\n",
    "* Los DataFrame <strong>no tienen tipo</strong>: Scala no puede inferir el tipo de sus elementos, porque los Dataframe están compuestos por <strong>filas</strong> (sin tipo)\n",
    "\n",
    "\n",
    "* Perdimos la flexibilidad de los RDD y los tipos y funciones definidos por el programador, contra un conjunto de tipos predefinidos (<em>Int, Long, String...</em>) y funciones relacionales (<em>SELECT, COUNT, WHERE...</em>)\n",
    "\n",
    "\n",
    "* Por otro lado, obtenemos enormes <strong>optimizaciones</strong> en términos de complejidad de tiempo gracias a estas fuertes restricciones.\n",
    "\n",
    "\n",
    "* Catalyst es el componente Spark a cargo de las optimizaciones de esos métodos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los dataframes se pueden crear leyendo directamente de un archivo de texto, usando la variable SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[authorId: string, date: string ... 3 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"test.txt\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authorId: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- idOriginal: string (nullable = true)\n",
      " |-- idTweet: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema                                // Imprime el schema del DataFrame, inferido del fichero Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "|  authorId|                date|        idOriginal|           idTweet|                text|\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "|2885455811|Thu Oct 05 08:52:...|915523419281739776|915831976929714177|RT @Societatcc: A...|\n",
      "|   2099361|Thu Oct 05 08:52:...|                  |915831940745441280|Yo ya he escogido...|\n",
      "| 799792832|Thu Oct 05 08:52:...|915830958443687936|915831968301973504|RT @pedroveraOyP:...|\n",
      "| 105157939|Thu Oct 05 08:52:...|915523419281739776|915831985582612480|RT @Societatcc: A...|\n",
      "| 124248712|Thu Oct 05 08:52:...|915830958443687936|915832004658286593|RT @pedroveraOyP:...|\n",
      "| 110117638|Thu Oct 05 08:48:...|                  |915830958443687936|#AmicsAmigos no p...|\n",
      "| 150587014|Thu Oct 05 08:52:...|915830945785237504|915832008936509440|RT @gsemprunmdg: ...|\n",
      "| 273360453|Thu Oct 05 08:52:...|915808416639143936|915832057288433664|RT @carmouna: Si ...|\n",
      "| 184865048|Thu Oct 05 07:18:...|                  |915808416639143936|Si no lo arreglan...|\n",
      "| 142775869|Thu Oct 05 09:10:...|                  |915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show                                       // Imprime los primeros 20 elementos en el DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los dataframes pueden ser creados a partir de un RDD existente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91580...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dfFromRawRDD = originalRDD.toDF                // Función importada de spark.implicits._\n",
    "dfFromRawRDD.printSchema\n",
    "dfFromRawRDD.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[487] at map at <console>:102"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Tweet(idTweet:Long, text:String, date:String, AuthorId:Long, idOriginal:Long)\n",
    "\n",
    "val formattedRDD = originalRDD.map(line => line.replace(\"{\", \"\").replace(\"}\", \"\")).\n",
    "    map(line => line.split(\"\\\",\\\"\")).\n",
    "    map(columns => columns.map(e => e.split(\"\\\":\\\"\")(1).replace(\"\\\"\",\"\"))).\n",
    "    map(attributes => {\n",
    "        // Warning: idOriginal puede estar vacío!!\n",
    "        val idorig = if(attributes(4)==\"\")  0 else attributes(4).toLong\n",
    "        \n",
    "        Tweet(attributes(0).toLong, attributes(1), attributes(2), attributes(3).toLong, idorig)\n",
    "    })\n",
    "formattedRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Este método no funciona bien en el <strong>notebook de Jupyter</strong></li>\n",
    "                <li>Compruébalo en el laboratorio</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los dataframes se pueden crear a partir de un RDD, especificando un schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Leer fichero de texto plano\n",
    "val originalRDD = spark.sparkContext.textFile(\"test.txt\")\n",
    "\n",
    "// Generar el schema especificando cada campo y su tipo\n",
    "val fields = List(\n",
    "  StructField(\"idTweet\", LongType, nullable = true),\n",
    "  StructField(\"text\", StringType, nullable = true),\n",
    "  StructField(\"date\", StringType, nullable = true),\n",
    "  StructField(\"authorId\", LongType, nullable = true),\n",
    "  StructField(\"idOriginal\", LongType, nullable = true))\n",
    "\n",
    "val schema = StructType(fields)\n",
    "\n",
    "// Leer el RDD de un fichero de texto\n",
    "val rowRDD = originalRDD.map(line => line.replace(\"{\", \"\").replace(\"}\", \"\")).\n",
    "  map(line => line.split(\"\\\",\\\"\")).\n",
    "  map(columns => columns.map(e => e.split(\"\\\":\\\"\")(1).replace(\"\\\"\",\"\"))).\n",
    "  map(attributes => {\n",
    "    // Warning: idOriginal puede estar vacío!!\n",
    "    val idOrig = if(attributes(4)==\"\") 0 else attributes(4).toLong\n",
    "\n",
    "    Row(attributes(0).toLong, attributes(1), attributes(2), attributes(3).toLong, idOrig)\n",
    "  })\n",
    "\n",
    "// Aplicar el schema al RDD\n",
    "val tweetDF = spark.createDataFrame(rowRDD, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jugando con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los DataFrames pueden usarse casi como una base de datos relacional SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "| authorId|                date|idOriginal|           idTweet|                text|\n",
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "|  2099361|Thu Oct 05 08:52:...|          |915831940745441280|Yo ya he escogido...|\n",
      "|110117638|Thu Oct 05 08:48:...|          |915830958443687936|#AmicsAmigos no p...|\n",
      "|184865048|Thu Oct 05 07:18:...|          |915808416639143936|Si no lo arreglan...|\n",
      "|142775869|Thu Oct 05 09:10:...|          |915836526789046273|La elegancia del ...|\n",
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Registrar el DataFrame como una vista SQL temporal\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "// Seleccionar tweets que NO son retweets\n",
    "val originalTweetsQueryDF\n",
    "= spark.sql(\"SELECT * FROM tweets WHERE idOriginal LIKE ''\")\n",
    "\n",
    "originalTweetsQueryDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El equivalente, usando funciones Spark SQL y $_notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|idOriginal|                date| authorId|           idTweet|                text|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|          |Thu Oct 05 08:52:...|  2099361|915831940745441280|Yo ya he escogido...|\n",
      "|          |Thu Oct 05 08:48:...|110117638|915830958443687936|#AmicsAmigos no p...|\n",
      "|          |Thu Oct 05 07:18:...|184865048|915808416639143936|Si no lo arreglan...|\n",
      "|          |Thu Oct 05 09:10:...|142775869|915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|idOriginal|                date| authorId|           idTweet|                text|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|          |Thu Oct 05 08:52:...|  2099361|915831940745441280|Yo ya he escogido...|\n",
      "|          |Thu Oct 05 08:48:...|110117638|915830958443687936|#AmicsAmigos no p...|\n",
      "|          |Thu Oct 05 07:18:...|184865048|915808416639143936|Si no lo arreglan...|\n",
      "|          |Thu Oct 05 09:10:...|142775869|915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)           // Los notebooks de Jupyter requieren estas frases\n",
    "import sqlC.implicits._                                      // para utilizar la $_notation\n",
    "\n",
    "// Seleccionar tweets que NO son retweets\n",
    "// $\"colName\" => acceso a la columna colName del DataFrame\n",
    "df.select($\"idOriginal\", $\"date\", $\"authorId\", $\"idTweet\", $\"text\").where(\"idOriginal LIKE''\").show\n",
    "\n",
    "// Alternativa:\n",
    "df.select($\"idOriginal\", $\"date\", $\"authorId\", $\"idTweet\", $\"text\").filter($\"idOriginal\".like(\"\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Fuera de los <strong>notebooks de Jupyter</strong> debemos reemplazar los primeros import por: <em>import spark.implicits._</em></li>\n",
    "                <li><em>spark</em> es el conector de la Spark Session.</li>\n",
    "                <li>Compruébalo en el laboratorio.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Spark SQL proporciona funciones equivalentes a la directivas SQL: <em>where, like, select, count</em>...</li>\n",
    "                <li>La <strong>$_notation</strong> nos permite acceder a las columnas de un DataFrame por sus nombres.</li>\n",
    "                <li>Las funciones de la Spark API, como <em>filter</em>, están también sobreescritas en la Spark SQL API, con el objetivo de aplicar las optimizaciones donde sea posible.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregaciones:\n",
    "\n",
    "* Una de las tareas más comunes con las bases de datos relacionales es agrupar y/o agregar atributos con ciertas condiciones para realizarle algunas acciones al resultado, como contar, sumar, calcular la media, etc.\n",
    "\n",
    "\n",
    "* Spark SQL proporciona la función <strong>groupBy</strong>, que devuelve un <em>RelationalGroupedDataset</em>\n",
    "\n",
    "\n",
    "* Este tipo tiene una serie de funciones de agregación relacional: sum, count, avg, max, min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.RelationalGroupedDataset@222e7463"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._       // NECESARIO para las funciones de agrupación\n",
    "\n",
    "// Ejemplo de agrupación:\n",
    "val grouped = df.groupBy($\"idOriginal\")\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idOriginal: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Contando por idOriginal:\n",
    "val groupedCount = grouped.count\n",
    "groupedCount.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        idOriginal|count|\n",
      "+------------------+-----+\n",
      "|915830945785237504|    1|\n",
      "|915808416639143936|    1|\n",
      "|915830958443687936|    2|\n",
      "|                  |    4|\n",
      "|915523419281739776|    2|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        idOriginal|count|\n",
      "+------------------+-----+\n",
      "|                  |    4|\n",
      "|915523419281739776|    2|\n",
      "|915830958443687936|    2|\n",
      "|915808416639143936|    1|\n",
      "|915830945785237504|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Ordenando los resultados\n",
    "groupedCount.orderBy($\"count\".desc).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(count)|\n",
      "+----------+\n",
      "|       2.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Average, max, min...\n",
    "groupedCount.agg(avg($\"count\")).show\n",
    "groupedCount.agg(max($\"count\")).show\n",
    "groupedCount.agg(min($\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subsubseccion-DataSets\"></a>\n",
    "### DataSets\n",
    "\n",
    "\n",
    "* Resumiendo: DataFrames con tipo\n",
    "\n",
    "\n",
    "* Los DataSets son una versión <strong>con tipo</strong> de los DataFrames: tenemos que especificar los tipos de cada columna en un DataSet.\n",
    "\n",
    "\n",
    "* En realidad: DataFrame = DataSet\\[Row\\]\n",
    "\n",
    "\n",
    "* Recuperamos la <strong>flexibilidad</strong> de los RDD y los tipos y funciones definidos por el programador, pero también manteniendo los tipos predefinidos en <strong>SparkSQL</strong> (<em>Int, Long, String...</em>) y las funciones relacionales (<em>SELECT, COUNT, WHERE...</em>)\n",
    "\n",
    "\n",
    "* Por otro lado, obtenemos <strong>parte</strong> de las optimizaciones de los DataFrames.\n",
    "\n",
    "\n",
    "* DataSets pueden verse como un compromiso entre los RDD y los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los DataSets pueden crearse a partir de un RDD existente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.createDataset(originalRDD)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los DataSets pueden crearse a partir de un DataFrame existente a través de una <strong>conversión de tipos a medida</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idTweet: long (nullable = false)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- AuthorId: long (nullable = false)\n",
      " |-- idOriginal: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val tweetDs = df.map(row => {\n",
    "    // Warning: idOriginal puede estar vacío!!\n",
    "    val idorig = if(row.getAs[String](\"idOriginal\")==\"\") 0 else row.getAs[String](\"idOriginal\").toLong\n",
    "    \n",
    "    Tweet(row.getAs[String](\"idTweet\").toLong, \n",
    "          row.getAs[String](\"text\"), \n",
    "          row.getAs[String](\"date\"), \n",
    "          row.getAs[String](\"authorId\").toLong, \n",
    "          idorig)\n",
    "})\n",
    "\n",
    "tweetDs.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Este método no funciona bien en los <strong>notebook de Jupyter</strong></li>\n",
    "                <li>Compruébalo en el laboratorio.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los DataSets pueden crearse a partir de un DataFrame existente mediante una <strong>conversión de tipos implícita</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idTweet: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- authorId: long (nullable = true)\n",
      " |-- idOriginal: long (nullable = true)\n",
      "\n",
      "+------------------+--------------------+--------------------+----------+------------------+\n",
      "|           idTweet|                text|                date|  authorId|        idOriginal|\n",
      "+------------------+--------------------+--------------------+----------+------------------+\n",
      "|915831976929714177|RT @Societatcc: A...|Thu Oct 05 08:52:...|2885455811|915523419281739776|\n",
      "|915831940745441280|Yo ya he escogido...|Thu Oct 05 08:52:...|   2099361|                 0|\n",
      "|915831968301973504|RT @pedroveraOyP:...|Thu Oct 05 08:52:...| 799792832|915830958443687936|\n",
      "|915831985582612480|RT @Societatcc: A...|Thu Oct 05 08:52:...| 105157939|915523419281739776|\n",
      "|915832004658286593|RT @pedroveraOyP:...|Thu Oct 05 08:52:...| 124248712|915830958443687936|\n",
      "|915830958443687936|#AmicsAmigos no p...|Thu Oct 05 08:48:...| 110117638|                 0|\n",
      "|915832008936509440|RT @gsemprunmdg: ...|Thu Oct 05 08:52:...| 150587014|915830945785237504|\n",
      "|915832057288433664|RT @carmouna: Si ...|Thu Oct 05 08:52:...| 273360453|915808416639143936|\n",
      "|915808416639143936|Si no lo arreglan...|Thu Oct 05 07:18:...| 184865048|                 0|\n",
      "|915836526789046273|La elegancia del ...|Thu Oct 05 09:10:...| 142775869|                 0|\n",
      "+------------------+--------------------+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val ds = tweetDF.as[Tweet]\n",
    "ds.printSchema\n",
    "ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jugando con DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|             value|count(1)|\n",
      "+------------------+--------+\n",
      "|                 0|       4|\n",
      "|915808416639143936|       1|\n",
      "|915523419281739776|       2|\n",
      "|915830958443687936|       2|\n",
      "|915830945785237504|       1|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.groupByKey(t => t.idOriginal).                // RDD API!!\n",
    "    count.show                                   // DataFrames API!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "| @gsemprunmdg|\n",
      "|@pedroveraOyP|\n",
      "|    @carmouna|\n",
      "| @radio3_rne…|\n",
      "|  @Societatcc|\n",
      "+-------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|        value|count|\n",
      "+-------------+-----+\n",
      "| @gsemprunmdg|    1|\n",
      "|@pedroveraOyP|    2|\n",
      "|    @carmouna|    1|\n",
      "| @radio3_rne…|    2|\n",
      "|  @Societatcc|    2|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val mentions = ds.flatMap(t => t.text.split(\" \").map(w => w.replaceAll(\":$\",\"\"))).filter(text => text.startsWith(\"@\"))\n",
    "\n",
    "mentions.distinct.show\n",
    "mentions.groupBy($\"value\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|        value|count(value)|\n",
      "+-------------+------------+\n",
      "| @gsemprunmdg|           1|\n",
      "|@pedroveraOyP|           2|\n",
      "|    @carmouna|           1|\n",
      "| @radio3_rne…|           2|\n",
      "|  @Societatcc|           2|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.groupBy($\"value\").agg(count($\"value\").as[Double]).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|             value|count(idTweet)|\n",
      "+------------------+--------------+\n",
      "|                 0|             4|\n",
      "|915808416639143936|             1|\n",
      "|915523419281739776|             2|\n",
      "|915830958443687936|             2|\n",
      "|915830945785237504|             1|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val groupedMentions = ds.groupByKey(tweet => tweet.idOriginal)\n",
    "\n",
    "groupedMentions.agg(count($\"idTweet\").as[Double]).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subsubseccion-Uso de RDDs DataFrames y Datasets\"></a>\n",
    "### Uso de RDDs, DataFrames y Datasets\n",
    "\n",
    "\n",
    "Entonces, ¿dónde debería usar RDD, Datasets o DataFrames en mi aplicación? Resumiremos las características de cada estructura de datos. Deberías usar...\n",
    "\n",
    "\n",
    "* RDD cuando...\n",
    "\n",
    "    - tus datos no están estructurados, por ejemplo, flujos binarios (media) o flujos de texto\n",
    "    - deseas controlar tu conjunto de datos y usar transformaciones y acciones de bajo nivel\n",
    "    - está bien pasar por alto las optimizaciones para DataFrames y Datasets para datos estructurados y semiestructurados que están disponibles de manera inmediata\n",
    "    - no te importa el schema, el formato de columna y las construcciones de programación funcional que están listas para su uso\n",
    "\n",
    "\n",
    "* DataFrames cuando...\n",
    "\n",
    "    - tus datos están estructurados (entrada RDBMS) o semiestructurados (json, csv)\n",
    "    - deseas obtener el mejor rendimiento obtenido del motor de ejecución optimizado de SQL\n",
    "    - necesitas ejecutar consultas de hive\n",
    "    - aprecias la API de 'domain specific language' (.groupBy, .agg, .orderBy)\n",
    "    - estás usando R o Python\n",
    "\n",
    "\n",
    "* DataSets cuando...\n",
    "\n",
    "    - tus datos están estructurados o semiestructurados\n",
    "    - aprecias la seguridad de tipos en tiempo de compilación y una API fuertemente tipada\n",
    "    - necesitas un buen rendimiento (sobre todo mayor que con RDD), pero no el mejor (generalmente más bajo que con DataFrames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
