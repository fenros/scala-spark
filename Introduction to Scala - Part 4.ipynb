{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/intro-logo-scala-eng.png\" align=\"left\" width=600px/>\n",
    "<!--- ![alt text](heading.png \"Heading with Scala logo\") --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "### [4. There's nothing left but... fly!](#seccion-4. Theres nothing left but... fly!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<!---\n",
    "# 4. There's nothing left but... fly! --->\n",
    "<a name=\"seccion-4. Theres nothing left but... fly!\"></a>\n",
    "<a name=\"seccion-4. There's nothing left but... fly!\"></a>\n",
    "<table align=\"left\" style=\"border-collapse: collapse; width: 100%; border: 5px double black\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 100px;\">\n",
    "<img src=\"icons/volar-m.png\" align=\"left\" width=85px/>\n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left;\">\n",
    "<h1>4. There's nothing left but... fly!</h1>\n",
    "<br>\n",
    "\n",
    "Having studied the syntax and the main features of Scala, there is nothing left but putting this knowledge into practice to solve more advance and interesting tasks.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "<a name=\"subseccion-Introduccion a Spark\"></a>\n",
    "## Introduction to Spark\n",
    "\n",
    "<!--<img src=\"images/intro_spark.png\" width=\"60%\" align=\"left\">-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "In this notebook we present the basics of Spark with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "#### Up to now:\n",
    "Data science and analytics has been done “in the small”, in R/Python/MATLAB, etc.\n",
    "\n",
    "\n",
    "#### Nowadays:\n",
    "Datasets do not fit into memory anymore, so...\n",
    "\n",
    "* These languages/frameworks won’t allow you to scale. \n",
    "\n",
    "* You have to reimplement everything in some other language or system.\n",
    "\n",
    "\n",
    "\n",
    "#### Moreover:\n",
    "\n",
    "* Industry is shifting towards Bussiness Intelligence based on data-oriented decision making, relying on huge datasets.\n",
    "\n",
    "* Spark's API is almost 1-to-1 with Scala's collections, but distributed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "### Spark + Scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "* More expressive. APIs modeled after Scala collections. Looks like functional lists! \n",
    "\n",
    "\n",
    "* Richer, more composable operations possible than in MapReduce (Hadoop).\n",
    "\n",
    "\n",
    "* Performant: in terms of running time... AND also in terms of developer productivity! \n",
    "\n",
    "\n",
    "* Good for data science. Not just because of performance, but because it enables (efficient) iteration, required by most algorithms in a data scientist’s toolbox.\n",
    "\n",
    "\n",
    "* High demand of Spark and Scala developers and Data Scientist!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "### Spark vs Hadoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "\n",
    "\n",
    "* Hadoop is an open source implementations of Google's MapReduce.\n",
    "\n",
    "\n",
    "* Simple API for map and reduce operations on distributed datasets.\n",
    "\n",
    "\n",
    "* Fault tolerance: between each map and reduce operations, writes intermediate data to be able to recover from failures.\n",
    "\n",
    "\n",
    "* Spark's fault tolerance is way more efficient because:\n",
    "    - Keeps all data inmutable and in-memory\n",
    "    - Operations are functional transformations\n",
    "    - Fault tolerance: re-aply transformations to original data\n",
    "    \n",
    "    \n",
    "* Spark is compatible with HDFS (Hadoop Distributed FileSystem)\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "header"
    ]
   },
   "source": [
    "<a name=\"subseccion-Key concepts in Spark\"></a>\n",
    "## Key concepts in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "text"
    ]
   },
   "source": [
    "* Spark Session: connection to Spark's API\n",
    "\n",
    "\n",
    "* Hardware Structure: \n",
    "    - Cluster of driver + workers\n",
    "    - Workflow: shuffling\n",
    "\n",
    "\n",
    "* Logical Data Structures:\n",
    "    - RDDs\n",
    "    - PairRDDs\n",
    "    \n",
    "    \n",
    "* Basic Operations:\n",
    "    - Transformations\n",
    "    - Actions\n",
    "    \n",
    "    \n",
    "* Interesting Libraries:\n",
    "    - Spark SQL: DataFrames and Datasets\n",
    "    - Spark Streaming API\n",
    "    - MlLib\n",
    "    - GraphX\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "### Spark Session\n",
    "\n",
    "Connection to the Spark cluster. \n",
    "\n",
    "Usually we \"talk to\" the master node of the cluster, and it sends the jobs to the worker nodes.\n",
    "\n",
    "SparkSession is the object that we will use to perform the configuration and input operations against the cluster.\n",
    "\n",
    "#### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:17: error: not found: value SparkSession\n",
       "       val spark = SparkSession\n",
       "                   ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Spark basic example\")     // Name for the session\n",
    "      .master(\"local[2]\")                 // Path and number of cores to be used\n",
    "      .getOrCreate()\n",
    "\n",
    "// Optional: Set logging level if log4j not configured\n",
    "Logger.getRootLogger.setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Previous versions of Spark use <strong>SparkContext</strong> instead of <strong>SparkSession</strong></li>\n",
    "                <li><strong>SparkContext</strong> is still in use, but its transparent to the developper</li>\n",
    "                <li>It can be accessed through the <strong>SparkSession</strong>: <strong>spark.sparkContext</strong></li>\n",
    "                <li>In this notebook, we already have both, a <strong>SparkSession</strong> and its corresponding <strong>SparkContext</strong> in the inmutable variables: <strong>spark</strong> and <strong>sc</strong>, respectively, which will be used during this notebook</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Hello World!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 100).reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>We use <strong>sc</strong> to build a parallel collection in Spark cluster.</li>\n",
    "                <li><strong>parallelize</strong> is a function to transform a collection to its correspondent parallel version.</li>\n",
    "                <li><strong>(1 to 100)</strong> is the definition of a range collection (a collection formed by the values within the given range)</li>\n",
    "                <li><strong>reduce</strong> has the same meaning as in Scala's Collection API</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities between Spark and Scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "Spark API has almost a 1-to-1 relation to Scala's collections API. Let's see an example:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María => 5\n",
      "Pedro => 5\n",
      "Elisa => 5\n",
      "Juan => 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lista = List(\"Juan\", \"María\", \"Pedro\", \"Elisa\")            // We build a Scala List[String]\n",
    "\n",
    "val paresLista = lista.map(nombre => (nombre, nombre.length))  // Associate the length of each string\n",
    "\n",
    "paresLista.sortBy(-_._2).foreach(t => println(t._1 + \" => \" + t._2))  // Print it out\n",
    "\n",
    "paresLista.map(_._2).reduce(_ + _)                             // Sum up the lengths of the strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent in Spark, in a distributed fashion, could be something like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María => 5\n",
      "Pedro => 5\n",
      "Elisa => 5\n",
      "Juan => 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parlista = sc.parallelize[String](lista)                      // Create the equivalent: ParallelCollectionRDD[String]\n",
    "\n",
    "val pares = parlista.map(nombre => (nombre, nombre.length))       // pares: MapPartitionRDD[(String, Int)]\n",
    "\n",
    "pares.sortBy(-_._2).collect.foreach(t => println(t._1 + \" => \" + t._2))\n",
    "\n",
    "pares.map(_._2).reduce(_ + _)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>1-to-1 correspondence between Scala and Spark</li>\n",
    "                <li><strong>collect</strong> brings the blocks of data distributed over the worker nodes to the master node, in order to process the whole RDD</li>\n",
    "                <li>Spark lazy evaluation of some functions (map, sortBy) and eager for others (reduce, collect). We will study it in depth bellow.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subseccion-Hardware Structure\"></a>\n",
    "## Hardware Structure in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/spark_structure.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Workflow\n",
    "\n",
    "* Master node distributes the data in blocks over the worker nodes, sends the tasks and integrates the results of the computation.\n",
    "\n",
    "\n",
    "* Worker nodes receive the data chunks and the tasks and perform the transformations and actions on their blocks of data.\n",
    "\n",
    "\n",
    "* Everytime our process requires the whole dataset to perform an action, the master node retrieves the data blocks from the workers, and reconstructs the data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>When the data travels trough the network is called <em>shuffle</em> and it is <strong>really expensive</strong></li>\n",
    "                <li>We must minimize the number of times that a <em>shuffle</em> is required by our application.</li>\n",
    "                <li>But take it easy by now: we need to know more Spark related concepts to thoroughly study this issue.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subseccion-Logical Data Structures RDDs\"></a>\n",
    "## Logical Data Structures: RDDs\n",
    "\n",
    "\n",
    "* Resilient Distributed Dataset\n",
    "\n",
    "\n",
    "* Parallel collections for distributed computation of functional programming\n",
    "\n",
    "\n",
    "* A collection of (typed) data, easily distributable over the worker nodes, so each node take charge of a chunk of the whole dataset to be processed.\n",
    "\n",
    "\n",
    "* An RDD is a logical reference of a dataset which is partitioned across many server machines in the Spark cluster.\n",
    "\n",
    "\n",
    "* RDD are partitioned and distributed over the workers in the Spark cluster automatically (without programmer intervention). See previous section of physical structure of a Spark cluster.\n",
    "\n",
    "\n",
    "* The partitioning scheme can be changed, but by default Spark tries to minimize the network traffic among nodes when processing the RDDs. Example: in a local environment, there is usually one partition per worker node (CPU cores available for Spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Example: Reading a Json file into an RDD\n",
    "\n",
    "\n",
    "Input Json file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[...]\n",
    "\n",
    "{\n",
    "    \"idTweet\":\"915831976929714177\",\n",
    "    \n",
    "    \"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\n",
    "    \n",
    "    \"date\":\"Thu Oct 05 08:52:13 CEST 2017\",\n",
    "    \n",
    "    \"authorId\":\"2885455811\",\n",
    "    \n",
    "    \"idOriginal\":\"915523419281739776\"\n",
    "}\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instructions in Spark to read the file and retrieve an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the file: 10\n",
      "Number of partitions in the RDD: 2\n"
     ]
    }
   ],
   "source": [
    "val testRDD = sc.textFile(\"test.txt\")            // Reads the file into a RDD\n",
    "val nRecords = testRDD.count                     // Returns the number of records read from the Json file\n",
    "val nPartitions = testRDD.partitions.size         // Returns the number of partitions of the testRDD\n",
    "\n",
    "\n",
    "println(\"Number of records in the file: \" + nRecords)\n",
    "println(\"Number of partitions in the RDD: \" + nPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's play a bit with our new RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Print the first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row: {\"idTweet\":\"915831976929714177\",\"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\"date\":\"Thu Oct 05 08:52:13 CEST 2017\",\"authorId\":\"2885455811\",\"idOriginal\":\"915523419281739776\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915831940745441280\",\"text\":\"Yo ya he escogido mediador. https://t.co/D7xS4MHbDG\",\"date\":\"Thu Oct 05 08:52:04 CEST 2017\",\"authorId\":\"2099361\",\"idOriginal\":\"\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915831968301973504\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:11 CEST 2017\",\"authorId\":\"799792832\",\"idOriginal\":\"915830958443687936\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915831985582612480\",\"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\"date\":\"Thu Oct 05 08:52:15 CEST 2017\",\"authorId\":\"105157939\",\"idOriginal\":\"915523419281739776\"}\n",
      "\n",
      "Row: {\"idTweet\":\"915832004658286593\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:19 CEST 2017\",\"authorId\":\"124248712\",\"idOriginal\":\"915830958443687936\"}\n"
     ]
    }
   ],
   "source": [
    "testRDD.take(5).foreach(elem => println(\"\\nRow: \" + elem ))    // Take 5 elems from the RDD and print them out on console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>This is just a part of the input file used in the upcoming examples.</li>\n",
    "                <li>With this instruction we read a Json as a plain text file.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Filter those tweets with any hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: {\"idTweet\":\"915831968301973504\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:11 CEST 2017\",\"authorId\":\"799792832\",\"idOriginal\":\"915830958443687936\"}\n",
      "Row: {\"idTweet\":\"915832004658286593\",\"text\":\"RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:52:19 CEST 2017\",\"authorId\":\"124248712\",\"idOriginal\":\"915830958443687936\"}\n",
      "Row: {\"idTweet\":\"915830958443687936\",\"text\":\"#AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\",\"date\":\"Thu Oct 05 08:48:10 CEST 2017\",\"authorId\":\"110117638\",\"idOriginal\":\"\"}\n",
      "Row: {\"idTweet\":\"915832008936509440\",\"text\":\"RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr\",\"date\":\"Thu Oct 05 08:52:20 CEST 2017\",\"authorId\":\"150587014\",\"idOriginal\":\"915830945785237504\"}\n",
      "Row: {\"idTweet\":\"915832057288433664\",\"text\":\"RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… \",\"date\":\"Thu Oct 05 08:52:32 CEST 2017\",\"authorId\":\"273360453\",\"idOriginal\":\"915808416639143936\"}\n",
      "Row: {\"idTweet\":\"915808416639143936\",\"text\":\"Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz\",\"date\":\"Thu Oct 05 07:18:36 CEST 2017\",\"authorId\":\"184865048\",\"idOriginal\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "val hashtagTweets = testRDD.filter(t => t.contains(\"#\"))\n",
    "hashtagTweets.collect.foreach(elem => println(\"Row: \" + elem ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Extract the ids of tweets and their authors and the original tweet (if it is a retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"idTweet\":\"915831976929714177, authorId\":\"2885455811, idOriginal\":\"915523419281739776\"}]\n",
      "[{\"idTweet\":\"915831940745441280, authorId\":\"2099361, idOriginal\":\"\"}]\n",
      "[{\"idTweet\":\"915831968301973504, authorId\":\"799792832, idOriginal\":\"915830958443687936\"}]\n",
      "[{\"idTweet\":\"915831985582612480, authorId\":\"105157939, idOriginal\":\"915523419281739776\"}]\n",
      "[{\"idTweet\":\"915832004658286593, authorId\":\"124248712, idOriginal\":\"915830958443687936\"}]\n",
      "[{\"idTweet\":\"915830958443687936, authorId\":\"110117638, idOriginal\":\"\"}]\n",
      "[{\"idTweet\":\"915832008936509440, authorId\":\"150587014, idOriginal\":\"915830945785237504\"}]\n",
      "[{\"idTweet\":\"915832057288433664, authorId\":\"273360453, idOriginal\":\"915808416639143936\"}]\n",
      "[{\"idTweet\":\"915808416639143936, authorId\":\"184865048, idOriginal\":\"\"}]\n",
      "[{\"idTweet\":\"915836526789046273, authorId\":\"142775869, idOriginal\":\"\"}]\n"
     ]
    }
   ],
   "source": [
    "val ids = testRDD.map(t => t.split(\"\\\",\\\"\")).map(fields => (fields(0), fields(3), fields(4)))\n",
    "\n",
    "ids.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Clean the elems of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[915831976929714177, 2885455811, 915523419281739776]\n",
      "[915831940745441280, 2099361, ]\n",
      "[915831968301973504, 799792832, 915830958443687936]\n",
      "[915831985582612480, 105157939, 915523419281739776]\n",
      "[915832004658286593, 124248712, 915830958443687936]\n",
      "[915830958443687936, 110117638, ]\n",
      "[915832008936509440, 150587014, 915830945785237504]\n",
      "[915832057288433664, 273360453, 915808416639143936]\n",
      "[915808416639143936, 184865048, ]\n",
      "[915836526789046273, 142775869, ]\n"
     ]
    }
   ],
   "source": [
    "val cleanIds = ids.map(tuple => {\n",
    "    (tuple._1.replace(\"{\\\"idTweet\\\":\\\"\", \"\"), \n",
    "    tuple._2.replace(\"authorId\\\":\\\"\",\"\"), \n",
    "    tuple._3.replace(\"idOriginal\\\":\\\"\",\"\").replace(\"\\\"}\",\"\"))\n",
    "    })\n",
    "\n",
    "cleanIds.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Convert the RDD[(String, String, String)] into an RDD[(Long, Long, Long)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 10.0 failed 1 times, most recent failure: Lost task 1.0 in stage 10.0 (TID 45, localhost, executor driver): java.lang.NumberFormatException: For input string: \"\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Long.parseLong(Long.java:601)\n",
       "\tat java.lang.Long.parseLong(Long.java:631)\n",
       "\tat scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)\n",
       "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:29)\n",
       "\tat $line44.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:28)\n",
       "\tat $line44.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:25)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Long.parseLong(Long.java:601)\n",
       "\tat java.lang.Long.parseLong(Long.java:631)\n",
       "\tat scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)\n",
       "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:29)\n",
       "\tat $anonfun$1.apply(<console>:28)\n",
       "\tat $anonfun$1.apply(<console>:25)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
       "  ... 40 elided\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"\"\n",
       "  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "  at java.lang.Long.parseLong(Long.java:601)\n",
       "  at java.lang.Long.parseLong(Long.java:631)\n",
       "  at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)\n",
       "  at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)\n",
       "  at $anonfun$1.apply(<console>:28)\n",
       "  at $anonfun$1.apply(<console>:25)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "  at scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val longIds = cleanIds.map(tuple => {\n",
    "    (tuple._1.toLong, \n",
    "    tuple._2.toLong, \n",
    "    tuple._3.toLong)\n",
    "    })\n",
    "\n",
    "longIds.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Try again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[915831976929714177, 2885455811, 915523419281739776]\n",
      "[915831940745441280, 2099361, 0]\n",
      "[915831968301973504, 799792832, 915830958443687936]\n",
      "[915831985582612480, 105157939, 915523419281739776]\n",
      "[915832004658286593, 124248712, 915830958443687936]\n",
      "[915830958443687936, 110117638, 0]\n",
      "[915832008936509440, 150587014, 915830945785237504]\n",
      "[915832057288433664, 273360453, 915808416639143936]\n",
      "[915808416639143936, 184865048, 0]\n",
      "[915836526789046273, 142775869, 0]\n"
     ]
    }
   ],
   "source": [
    "val longIds = cleanIds.map(tuple => {\n",
    "    (tuple._1.toLong, \n",
    "    tuple._2.toLong, \n",
    "    if(tuple._3 == \"\") 0 else tuple._3.toLong)\n",
    "    })\n",
    "\n",
    "longIds.collect.foreach(elem => println(\"[\" + elem._1 + \", \" + elem._2 + \", \" + elem._3 + \"]\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Retrieve tweets that have been retweeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915830958443687936,110117638,0)\n",
      "(915808416639143936,184865048,0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(915523419281739776, 915830945785237504, 915830958443687936, 915808416639143936)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retweetedIDS = longIds.filter(t => t._3!=0).map(_._3).distinct.collect\n",
    "\n",
    "val original = longIds.filter(t => retweetedIDS.contains(t._1))\n",
    "\n",
    "\n",
    "original.collect.foreach(println _)\n",
    "retweetedIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subseccion-Logical Data Structures PairRDDs\"></a>\n",
    "## Logical Data Structures: PairRDDs\n",
    "\n",
    "\n",
    "* Intuition: parallel distributed version of a `map`.\n",
    "\n",
    "\n",
    "* An RDD containing tuples of (key, value)\n",
    "\n",
    "\n",
    "* Useful because `maps` are one of the most used data abstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case of PairRDDs: Counting words in an RDD\n",
    "\n",
    "\n",
    "1.- First: let's split the content of the RDD into words: using <strong>flatMap</strong>\n",
    "\n",
    "2.- Then, produce a PairRDD with: (Word, 1): using <strong>map</strong>\n",
    "\n",
    "3.- Finally, group each pair according to their first component (the word) and sum up the second components (occurrences of the words): using <strong>reduceByKey</strong> function of PairRDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas leídas: 101\n",
      "Word: #ranciofacts\tOccurrences: 3\n",
      "Word: recodo\tOccurrences: 1\n",
      "Word: 2017\",\"authorId\":\"124248712\",\"idOriginal\":\"915830958443687936\"}\tOccurrences: 1\n",
      "Word: arreglan\tOccurrences: 2\n",
      "Word: https://t.co/trDDTvrjgr\",\"date\":\"Thu\tOccurrences: 1\n",
      "Word: Si\tOccurrences: 1\n",
      "Word: donde\tOccurrences: 1\n",
      "Word: ya\tOccurrences: 1\n",
      "Word: los\tOccurrences: 4\n",
      "Word: {\"idTweet\":\"915830958443687936\",\"text\":\"#AmicsAmigos\tOccurrences: 1\n",
      "Word: 2017\",\"authorId\":\"142775869\",\"idOriginal\":\"\"}\tOccurrences: 1\n",
      "Word: x\tOccurrences: 1\n",
      "Word: @pedroveraOyP:\tOccurrences: 2\n",
      "Word: #AmicsAmigos\tOccurrences: 2\n",
      "Word: pero...\tOccurrences: 2\n"
     ]
    }
   ],
   "source": [
    "// Common mechanism of count elements by mapping a RDD to a PairRDD\n",
    "val countWords = testRDD.flatMap(line => line.split(\" \")).map(w => (w, 1)).reduceByKey(_ + _)\n",
    "\n",
    "println(\"Filas leídas: \" + countWords.count)\n",
    "\n",
    "// Printing it out\n",
    "countWords.take(15).foreach(t => println(\"Word: \" + t._1 + \"\\tOccurrences: \" + t._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's think it twice...\n",
    "\n",
    "\n",
    "* The first step goes from an `RDD[String]` to an `RDD[String]`: **flatMap** splits each <em>Tweet</em> into a Collection[*words*], and then flats them, obtaining an RDD[<em>words</em>].\n",
    "\n",
    "\n",
    "* The second step goes from an `RDD[String]`, where each String is a word, to an `RDD[(String, Int)]`, that is a `PairRDD[(String, Int)]`.\n",
    "\n",
    "* Finally, **reduceByKey** groups all the tuples with the same word, summing up their values, producing a `PairRDD[(String, Int)]` that represents an RDD[*(word, occurrencesOfWord)*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>In the example above, we take each line of the Json file as a String.</li>\n",
    "                <li>That means that we are counting all the words in the texts, including the names of the Json fields, and things that are not just texts.</li>\n",
    "                <li>How can we fix it? Let's see it in next sections.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we really doing?\n",
    "\n",
    "\n",
    "When we read a file with <strong>textFile</strong>, Spark creates an RDD[String]. It doesn't infer the structure of the Json, nor the pairs (field, value).\n",
    "\n",
    "\n",
    "The first idea to overcome this could be something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val texts = testRDD.map(row => row.split(\"\\\",\")).map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the instruction above do with the original RDD[String]?\n",
    "\n",
    "Let's print it out and see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "Yo ya he escogido mediador. https://t.co/D7xS4MHbDG\n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n"
     ]
    }
   ],
   "source": [
    "texts.take(5).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can count the words of each text following the same procedure as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val countWordsTexts = texts.flatMap(line => line.split(\" \")).map(w => (w, 1)).reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \tOccurrences: 8\n",
      "Word: a\tOccurrences: 8\n",
      "Word: no\tOccurrences: 7\n",
      "Word: RT\tOccurrences: 6\n",
      "Word: que\tOccurrences: 5\n",
      "Word: los\tOccurrences: 4\n",
      "Word: tenemos\tOccurrences: 4\n",
      "Word: lo\tOccurrences: 4\n",
      "Word: todos\tOccurrences: 4\n",
      "Word: #ranciofacts\tOccurrences: 3\n",
      "Word: https://t.co/mjMhHQfHuB\tOccurrences: 3\n",
      "Word: #AmicsAmigos\tOccurrences: 3\n",
      "Word: es\tOccurrences: 3\n",
      "Word: pelearsen\tOccurrences: 3\n",
      "Word: muy\tOccurrences: 3\n"
     ]
    }
   ],
   "source": [
    "// Printing it out, sorted by the counting\n",
    "countWordsTexts.sortBy(-_._2).take(15).foreach(t => println(\"Word: \" + t._1 + \"\\tOccurrences: \" + t._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pros of RDDs and PairRDDs\n",
    "\n",
    "\n",
    "    1.- Easy to use API, based on Scala's collections API (map, reduce, filter, flatMap...)\n",
    "    \n",
    "    2.- Optimized to use in a distributed Spark cluster\n",
    "    \n",
    "    3.- Typed collections: relying on the Scala type inference\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### But, really... it is a bit tedious, isn't it?\n",
    "\n",
    "\n",
    "    1.- Not good for processing structured or semi-structured data: \n",
    "        - In the example, we tried to read a <strong>structured</strong> file, in Json format\n",
    "        - So we lost all that <strong>precious information</strong> (fields, values, etc.) transforming it into a (resilient and distributable) collection of strings.\n",
    "        - And then use the same old <strong>split-get-replace</strong> boring stuff in the String class to extract the interesting parts from the string.\n",
    "\n",
    "\n",
    "    2.- Shuffling can become the bottle-neck of our application and sometimes it is not easy to avoid it.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>In relation to <strong>data shuffle</strong>, in next section we will study the basic operations in Spark (<em>Transformations</em> and <em>Actions</em>), their effects and the way they are managed within the Spark cluster.</li>\n",
    "                <li>Regarding the processing of <strong>structured and semi-structured information</strong>: Spark offers a much better way of dealing with this kind of data through the <em>Spark SQL</em> library and its relational data structures: <em>DataFrames</em> and <em>Datasets</em>. We will study them later.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subseccion-Transformations and actions\"></a>\n",
    "## Spark Basic Operations: Transformations and Actions\n",
    "\n",
    "\n",
    "\n",
    "Apache Spark RDDs support two types of operations: Transformations and Actions.\n",
    "\n",
    "\n",
    "\n",
    "### Transformations\n",
    "\n",
    "\n",
    "* They are functions that produce new RDDs from the existing ones. Examples: map(), filter().\n",
    "\n",
    "\n",
    "* Since the input RDDs cannot be changed (they are immutable in nature), every time we apply a transformation new RDDs are created. \n",
    "\n",
    "\n",
    "* Transformations are lazily evaluated, which means that they are not executed immediately. A transformation is effectively executed when we call an action.\n",
    "\n",
    "\n",
    "* So, applying a (number of) transformation(s) do not produce any inmediate effects. Instead, an RDD lineage is built up, going from the original RDD (which invokes the first transformation) to the final RDDs (result of all the transformations). RDD lineage, represented by a <strong>DAG</strong> (Directed Acyclic Graph), it's a logical execution plan of all the transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of transformations and their DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2) ShuffledRDD[30] at reduceByKey at <console>:23 []\n",
       " +-(2) MapPartitionsRDD[29] at map at <console>:23 []\n",
       "    |  MapPartitionsRDD[28] at flatMap at <console>:23 []\n",
       "    |  MapPartitionsRDD[27] at map at <console>:21 []\n",
       "    |  MapPartitionsRDD[26] at map at <console>:21 []\n",
       "    |  test.txt MapPartitionsRDD[10] at textFile at <console>:19 []\n",
       "    |  test.txt HadoopRDD[9] at textFile at <console>:19 []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remember the code in [7] and [9]: \n",
    "    // val texts = testRDD.map(row => row.split(\"\\\",\")).map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\"))\n",
    "    // val countWordsTexts = texts.flatMap(line => line.split(\" \")).map(w => (w, 1)).reduceByKey(_ + _)\n",
    "\n",
    "countWordsTexts.toDebugString            // print out the execution plan (DAG) of the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of transformations: \n",
    "\n",
    "\n",
    "\n",
    "* Narrow transformations: they do not imply a shuffle of data. They can be computed by each worker node with their own data partitions.\n",
    "    - Examples: map, filter, flatMap, union, sample...\n",
    "\n",
    "\n",
    "\n",
    "* Wide transformations: the processing logic depends on data from multiple partitions, so data shuffling is needed to bring them together in one place.\n",
    "    - Examples: distinct, join, reduceByKey, groupByKey...\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li> Spark implements a mechanism to optimize the execution plan of transformations in order to minimize the data shuffling. For example: it groups narrow transformations into one `stage`.</li>\n",
    "                <li>Remember that transformations are <strong>lazy</strong>: they are not executed when they are declared.</li>\n",
    "                <li>One way of actually perform a set of transformations is to apply an action to the output RDD.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>DAG is the mechanism that allows Spark to be fault-tolerant, <strong>without</strong> having to write data to disk as a backup</li>\n",
    "                <li>Spark recovers from failures by recomputing the lost partitions, following the <strong>DAG</strong></li>\n",
    "                <li>It is really <strong>fast</strong> to recover data from <strong>narrow</strong> transformations, but <strong>slow</strong> from <strong>wide</strong> transformations.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions: \n",
    "\n",
    "\n",
    "\n",
    "* They are Spark RDD operations that produce non-RDD values. \n",
    "\n",
    "\n",
    "* The results of actions are stored to driver nodes or to the external storage system. So an action is one of the ways of sending data from the worker nodes to the driver.\n",
    "\n",
    "\n",
    "* It brings laziness of RDD into motion, which means that an action provokes the execution of the associated transformations on the RDD.\n",
    "\n",
    "* Examples: count, collect, first, take..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review the example RDD from a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val originalRDD = sc.textFile(\"test.txt\")           // Read plain text file\n",
    "\n",
    "val firstTransformation = originalRDD.map(row => row.split(\"\\\",\"))\n",
    "\n",
    "val secondTransformation = firstTransformation.map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\"))\n",
    "\n",
    "val thirdTransformation = secondTransformation.filter(text => text.contains(\"@\"))\n",
    "\n",
    "val fourthTransformation = secondTransformation.flatMap(text => text.split(\" \"))\n",
    "\n",
    "val fifthTransformation = fourthTransformation.filter(word => word.startsWith(\"#\"))\n",
    "\n",
    "val sixthTransformation = fifthTransformation.map(_.toLowerCase).distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/question.jpg\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>What have we done, up to now?</li>\n",
    "                <li>What's the content of each RDD?</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr\n",
      "RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… \n",
      "Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n",
      "#amicsamigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n"
     ]
    }
   ],
   "source": [
    "thirdTransformation.collect.foreach(println)         // Transformation to be computed: 3, 2 and 1\n",
    "println\n",
    "fifthTransformation.collect.foreach(println)         // Transformation to be computed: 5, 4, 2 and 1\n",
    "println\n",
    "sixthTransformation.collect.foreach(println)         // Transformation to be computed: 6, 5, 4, 2 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Remember that transformations are lazily evaluated, so... </li>\n",
    "                <li>Notice that transformations 2 and 1 are evaluated three times!!</li>\n",
    "                <li>Spark provides a mechanism to help programmers to prevent this situation: <strong>caching</strong>. Let's rewrite our code:</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val originalRDD2 = sc.textFile(\"test.txt\")           // Read plain text file\n",
    "\n",
    "val firstT = originalRDD2.map(row => row.split(\"\\\",\"))\n",
    "\n",
    "val secondT = firstT.map(row => row(1).replace(\"\\\"text\\\":\\\"\", \"\")).cache    // Cache the result!!\n",
    "\n",
    "val thirdT = secondT.filter(text => text.contains(\"@\"))\n",
    "\n",
    "val fourthT = secondT.flatMap(text => text.split(\" \"))\n",
    "\n",
    "val fifthT = fourthT.filter(word => word.startsWith(\"#\"))\n",
    "\n",
    "val sixthT = fifthT.map(_.toLowerCase).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \n",
      "RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB\n",
      "RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr\n",
      "RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… \n",
      "Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#AmicsAmigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n",
      "#amicsamigos\n",
      "#ranciofacts\n",
      "#amicsamigos\n"
     ]
    }
   ],
   "source": [
    "thirdT.collect.foreach(println)         // Transformation to be computed: 3, 2 and 1, and caches the second transformation\n",
    "println\n",
    "fifthT.collect.foreach(println)         // Transformation to be computed: 5 and 4 over the already evaluated and cached 2\n",
    "println\n",
    "sixthT.collect.foreach(println)         // Transformation to be computed: 6, 5, 4 over the already evaluated and cached 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subseccion-Spark SQL\"></a>\n",
    "## Spark SQL: DataFrames and Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Spark SQL features\n",
    "\n",
    "\n",
    "* Spark library that integrates SQL-based syntax to perform operations on distributed data.\n",
    "\n",
    "\n",
    "* Defines data structures to ease the implementation of relational operations (select, group-by, order-by, max, min, average, count, etc.): DataFrames and Datasets.\n",
    "\n",
    "\n",
    "* These data structures integrates performance optimizations from SQL relational algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>In order to use the optimized syntax of Spark SQL we must include the following line of code: <em>import spark.implicits._</em></li>\n",
    "                <li>It is also useful to transform RDDs to DataFrames</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// In the Jupyter notebooks this line should be different\n",
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subsubseccion-DataFrames\"></a>\n",
    "### DataFrames\n",
    "\n",
    "\n",
    "* Conceptually equivalent to a SQL table\n",
    "\n",
    "\n",
    "* Dataframes are <strong>untyped</strong>: Scala cannot infer the type of its elements, because Dataframes are composed by <strong>Rows</strong> (without type)\n",
    "\n",
    "\n",
    "* We lost the flexibility of RDDs and programmer-defined types and functions, against a set of pre-defined types (<em>Int, Long, String...</em>) and relational functions (<em>SELECT, COUNT, WHERE...</em>)\n",
    "\n",
    "\n",
    "* On the other hand, we get enormous <strong>optimizations</strong> in terms of time complexity thanks to these strong constraints.\n",
    "\n",
    "\n",
    "* Catalyst is the Spark component in charge of the optimizations of those methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataframes can be created by reading directly from a text file, using the SparkSession variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[authorId: string, date: string ... 3 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"test.txt\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authorId: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- idOriginal: string (nullable = true)\n",
      " |-- idTweet: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema                                // Prints the schema of the DataFrame, inferred from the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "|  authorId|                date|        idOriginal|           idTweet|                text|\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "|2885455811|Thu Oct 05 08:52:...|915523419281739776|915831976929714177|RT @Societatcc: A...|\n",
      "|   2099361|Thu Oct 05 08:52:...|                  |915831940745441280|Yo ya he escogido...|\n",
      "| 799792832|Thu Oct 05 08:52:...|915830958443687936|915831968301973504|RT @pedroveraOyP:...|\n",
      "| 105157939|Thu Oct 05 08:52:...|915523419281739776|915831985582612480|RT @Societatcc: A...|\n",
      "| 124248712|Thu Oct 05 08:52:...|915830958443687936|915832004658286593|RT @pedroveraOyP:...|\n",
      "| 110117638|Thu Oct 05 08:48:...|                  |915830958443687936|#AmicsAmigos no p...|\n",
      "| 150587014|Thu Oct 05 08:52:...|915830945785237504|915832008936509440|RT @gsemprunmdg: ...|\n",
      "| 273360453|Thu Oct 05 08:52:...|915808416639143936|915832057288433664|RT @carmouna: Si ...|\n",
      "| 184865048|Thu Oct 05 07:18:...|                  |915808416639143936|Si no lo arreglan...|\n",
      "| 142775869|Thu Oct 05 09:10:...|                  |915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show                                       // Print the first 20 elements in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataframes can be created from an existing RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "|{\"idTweet\":\"91580...|\n",
      "|{\"idTweet\":\"91583...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dfFromRawRDD = originalRDD.toDF                // Function imported from spark.implicits._\n",
    "dfFromRawRDD.printSchema\n",
    "dfFromRawRDD.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/question.jpg\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>What are the difference between <em>df</em> and <em>dfFromRDD</em>?</li>\n",
    "                <li>Why is that so?</li>\n",
    "                <li>Let's solve it in the first Spark practice class</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[487] at map at <console>:102"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Tweet(idTweet:Long, text:String, date:String, AuthorId:Long, idOriginal:Long)\n",
    "\n",
    "val formattedRDD = originalRDD.map(line => line.replace(\"{\", \"\").replace(\"}\", \"\")).\n",
    "    map(line => line.split(\"\\\",\\\"\")).\n",
    "    map(columns => columns.map(e => e.split(\"\\\":\\\"\")(1).replace(\"\\\"\",\"\"))).\n",
    "    map(attributes => {\n",
    "        // Warning: idOriginal can be empty!!\n",
    "        val idorig = if(attributes(4)==\"\")  0 else attributes(4).toLong\n",
    "        \n",
    "        Tweet(attributes(0).toLong, attributes(1), attributes(2), attributes(3).toLong, idorig)\n",
    "    })\n",
    "formattedRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>This method does not work fine on <strong>Jupyter Notebook</strong></li>\n",
    "                <li>Check it in Lab.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataframes can be created from an RDD, specifying a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.17.0.2:4041\n",
       "SparkContext available as 'sc' (version = 2.2.0, master = local[*], app id = local-1516870159654)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "originalRDD: org.apache.spark.rdd.RDD[String] = test.txt MapPartitionsRDD[1] at textFile at <console>:28\n",
       "fields: List[org.apache.spark.sql.types.StructField] = List(StructField(idTweet,LongType,true), StructField(text,StringType,true), StructField(date,StringType,true), StructField(authorId,LongType,true), StructField(idOriginal,LongType,true))\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(idTweet,LongType,true), StructField(text,StringType,true), StructField(date,StringType,true), StructField(authorId,LongType,true), StructField(idOriginal,LongType,true))\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[5] at map at <console>:44\n",
       "tweetDF: org.apache.spark.sql.DataFrame..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Read plain text file\n",
    "val originalRDD = spark.sparkContext.textFile(\"test.txt\")\n",
    "\n",
    "// Generate the schema specifying each field and its type\n",
    "val fields = List(\n",
    "  StructField(\"idTweet\", LongType, nullable = true),\n",
    "  StructField(\"text\", StringType, nullable = true),\n",
    "  StructField(\"date\", StringType, nullable = true),\n",
    "  StructField(\"authorId\", LongType, nullable = true),\n",
    "  StructField(\"idOriginal\", LongType, nullable = true))\n",
    "\n",
    "val schema = StructType(fields)\n",
    "\n",
    "// Read the RDD from a text file\n",
    "val rowRDD = originalRDD.map(line => line.replace(\"{\", \"\").replace(\"}\", \"\")).\n",
    "  map(line => line.split(\"\\\",\\\"\")).\n",
    "  map(columns => columns.map(e => e.split(\"\\\":\\\"\")(1).replace(\"\\\"\",\"\"))).\n",
    "  map(attributes => {\n",
    "    // Warning: idOriginal can be empty!!\n",
    "    val idOrig = if(attributes(4)==\"\") 0 else attributes(4).toLong\n",
    "\n",
    "    Row(attributes(0).toLong, attributes(1), attributes(2), attributes(3).toLong, idOrig)\n",
    "  })\n",
    "\n",
    "// Apply the schema to the RDD\n",
    "val tweetDF = spark.createDataFrame(rowRDD, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing with DataFrames: Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be used almost like a SQL relational database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "| authorId|                date|idOriginal|           idTweet|                text|\n",
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "|  2099361|Thu Oct 05 08:52:...|          |915831940745441280|Yo ya he escogido...|\n",
      "|110117638|Thu Oct 05 08:48:...|          |915830958443687936|#AmicsAmigos no p...|\n",
      "|184865048|Thu Oct 05 07:18:...|          |915808416639143936|Si no lo arreglan...|\n",
      "|142775869|Thu Oct 05 09:10:...|          |915836526789046273|La elegancia del ...|\n",
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "// Select tweets that are NOT retweets\n",
    "val originalTweetsQueryDF\n",
    "= spark.sql(\"SELECT * FROM tweets WHERE idOriginal LIKE ''\")\n",
    "\n",
    "originalTweetsQueryDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The equivalent, using Spark SQL functions and $_notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|idOriginal|                date| authorId|           idTweet|                text|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|          |Thu Oct 05 08:52:...|  2099361|915831940745441280|Yo ya he escogido...|\n",
      "|          |Thu Oct 05 08:48:...|110117638|915830958443687936|#AmicsAmigos no p...|\n",
      "|          |Thu Oct 05 07:18:...|184865048|915808416639143936|Si no lo arreglan...|\n",
      "|          |Thu Oct 05 09:10:...|142775869|915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|idOriginal|                date| authorId|           idTweet|                text|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|          |Thu Oct 05 08:52:...|  2099361|915831940745441280|Yo ya he escogido...|\n",
      "|          |Thu Oct 05 08:48:...|110117638|915830958443687936|#AmicsAmigos no p...|\n",
      "|          |Thu Oct 05 07:18:...|184865048|915808416639143936|Si no lo arreglan...|\n",
      "|          |Thu Oct 05 09:10:...|142775869|915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)           // Jupyter Notebooks require these sentences\n",
    "import sqlC.implicits._                                      // in order to use $_notation\n",
    "\n",
    "// Select tweets that are NOT retweets\n",
    "// $\"colName\" => access to the colName of the DataFrame\n",
    "df.select($\"idOriginal\", $\"date\", $\"authorId\", $\"idTweet\", $\"text\").where(\"idOriginal LIKE''\").show\n",
    "\n",
    "// Alternative way:\n",
    "df.select($\"idOriginal\", $\"date\", $\"authorId\", $\"idTweet\", $\"text\").filter($\"idOriginal\".like(\"\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Outside <strong>Jupyter notebooks</strong> we must replace the first imports for: <em>import spark.implicits._</em></li>\n",
    "                <li><em>spark</em> is the Spark Session connector.</li>\n",
    "                <li>Check it in Lab.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Spark SQL provides functions equivalent to SQL directives: <em>where, like, select, count</em>...</li>\n",
    "                <li><strong>$_notation</strong> allows us to access to the columns of a DataFrame by their name.</li>\n",
    "                <li>Functions from Spark API, like <em>filter</em>, are also override in Spark SQL API, in order to apply the optimizations when possible.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations:\n",
    "\n",
    "* One of the most common tasks with relational databases is grouping and/or aggregating attributes with certain conditions to perform some actions to the result, such as counting, summing, averaging, etc.\n",
    "\n",
    "\n",
    "\n",
    "* Spark SQL provides the function <strong>groupBy</strong>, wich returns a <em>RelationalGroupedDataset</em>\n",
    "\n",
    "\n",
    "\n",
    "* This type has a number of relational aggregation functions: sum, count, avg, max, min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.RelationalGroupedDataset@222e7463"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._                   // NEEDED for grouping functions\n",
    "\n",
    "// Example of grouping:\n",
    "val grouped = df.groupBy($\"idOriginal\")\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idOriginal: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Counting by idOriginal:\n",
    "val groupedCount = grouped.count\n",
    "groupedCount.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        idOriginal|count|\n",
      "+------------------+-----+\n",
      "|915830945785237504|    1|\n",
      "|915808416639143936|    1|\n",
      "|915830958443687936|    2|\n",
      "|                  |    4|\n",
      "|915523419281739776|    2|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        idOriginal|count|\n",
      "+------------------+-----+\n",
      "|                  |    4|\n",
      "|915523419281739776|    2|\n",
      "|915830958443687936|    2|\n",
      "|915808416639143936|    1|\n",
      "|915830945785237504|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Sorting the results\n",
    "groupedCount.orderBy($\"count\".desc).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(count)|\n",
      "+----------+\n",
      "|       2.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Average, max, min...\n",
    "groupedCount.agg(avg($\"count\")).show\n",
    "groupedCount.agg(max($\"count\")).show\n",
    "groupedCount.agg(min($\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"subsubseccion-DataSets\"></a>\n",
    "### DataSets\n",
    "\n",
    "\n",
    "* In short: Typed DataFrames\n",
    "\n",
    "\n",
    "* DataSets are a <strong>typed</strong> version of DataFrames: we have to specify the types of each column in a DataSet.\n",
    "\n",
    "\n",
    "* Actually: DataFrame = DataSet\\[Row\\]\n",
    "\n",
    "\n",
    "* We recover the <strong>flexibility</strong> of RDDs and programmer-defined types and functions, but also preserving the <strong>SparkSQL</strong> pre-defined types (<em>Int, Long, String...</em>) and relational functions (<em>SELECT, COUNT, WHERE...</em>)\n",
    "\n",
    "\n",
    "* On the other hand, we get <strong>part</strong> of the optimizations of DataFrames.\n",
    "\n",
    "\n",
    "* DataSets can be seen as a compromise between RDDs and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSets can be created from an existing RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.createDataset(originalRDD)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSets can be created from an existing DataFrame by <strong>hand-made type conversion</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idTweet: long (nullable = false)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- AuthorId: long (nullable = false)\n",
      " |-- idOriginal: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val tweetDs = df.map(row => {\n",
    "    // Warning: idOriginal can be empty!!\n",
    "    val idorig = if(row.getAs[String](\"idOriginal\")==\"\") 0 else row.getAs[String](\"idOriginal\").toLong\n",
    "    \n",
    "    Tweet(row.getAs[String](\"idTweet\").toLong, \n",
    "          row.getAs[String](\"text\"), \n",
    "          row.getAs[String](\"date\"), \n",
    "          row.getAs[String](\"authorId\").toLong, \n",
    "          idorig)\n",
    "})\n",
    "\n",
    "tweetDs.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>This method does not work fine on <strong>Jupyter Notebook</strong></li>\n",
    "                <li>Check it in Lab.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSets can be created from an existing DataFrame by <strong>implicit type conversion</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idTweet: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- authorId: long (nullable = true)\n",
      " |-- idOriginal: long (nullable = true)\n",
      "\n",
      "+------------------+--------------------+--------------------+----------+------------------+\n",
      "|           idTweet|                text|                date|  authorId|        idOriginal|\n",
      "+------------------+--------------------+--------------------+----------+------------------+\n",
      "|915831976929714177|RT @Societatcc: A...|Thu Oct 05 08:52:...|2885455811|915523419281739776|\n",
      "|915831940745441280|Yo ya he escogido...|Thu Oct 05 08:52:...|   2099361|                 0|\n",
      "|915831968301973504|RT @pedroveraOyP:...|Thu Oct 05 08:52:...| 799792832|915830958443687936|\n",
      "|915831985582612480|RT @Societatcc: A...|Thu Oct 05 08:52:...| 105157939|915523419281739776|\n",
      "|915832004658286593|RT @pedroveraOyP:...|Thu Oct 05 08:52:...| 124248712|915830958443687936|\n",
      "|915830958443687936|#AmicsAmigos no p...|Thu Oct 05 08:48:...| 110117638|                 0|\n",
      "|915832008936509440|RT @gsemprunmdg: ...|Thu Oct 05 08:52:...| 150587014|915830945785237504|\n",
      "|915832057288433664|RT @carmouna: Si ...|Thu Oct 05 08:52:...| 273360453|915808416639143936|\n",
      "|915808416639143936|Si no lo arreglan...|Thu Oct 05 07:18:...| 184865048|                 0|\n",
      "|915836526789046273|La elegancia del ...|Thu Oct 05 09:10:...| 142775869|                 0|\n",
      "+------------------+--------------------+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val ds = tweetDF.as[Tweet]\n",
    "ds.printSchema\n",
    "ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing with DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|             value|count(1)|\n",
      "+------------------+--------+\n",
      "|                 0|       4|\n",
      "|915808416639143936|       1|\n",
      "|915523419281739776|       2|\n",
      "|915830958443687936|       2|\n",
      "|915830945785237504|       1|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.groupByKey(t => t.idOriginal).                // RDD API!!\n",
    "    count.show                                   // DataFrames API!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "| @gsemprunmdg|\n",
      "|@pedroveraOyP|\n",
      "|    @carmouna|\n",
      "| @radio3_rne…|\n",
      "|  @Societatcc|\n",
      "+-------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|        value|count|\n",
      "+-------------+-----+\n",
      "| @gsemprunmdg|    1|\n",
      "|@pedroveraOyP|    2|\n",
      "|    @carmouna|    1|\n",
      "| @radio3_rne…|    2|\n",
      "|  @Societatcc|    2|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val mentions = ds.flatMap(t => t.text.split(\" \").map(w => w.replaceAll(\":$\",\"\"))).filter(text => text.startsWith(\"@\"))\n",
    "\n",
    "mentions.distinct.show\n",
    "mentions.groupBy($\"value\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|        value|count(value)|\n",
      "+-------------+------------+\n",
      "| @gsemprunmdg|           1|\n",
      "|@pedroveraOyP|           2|\n",
      "|    @carmouna|           1|\n",
      "| @radio3_rne…|           2|\n",
      "|  @Societatcc|           2|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.groupBy($\"value\").agg(count($\"value\").as[Double]).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|             value|count(idTweet)|\n",
      "+------------------+--------------+\n",
      "|                 0|             4|\n",
      "|915808416639143936|             1|\n",
      "|915523419281739776|             2|\n",
      "|915830958443687936|             2|\n",
      "|915830945785237504|             1|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val groupedMentions = ds.groupByKey(tweet => tweet.idOriginal)\n",
    "\n",
    "groupedMentions.agg(count($\"idTweet\").as[Double]).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"subsubseccion-Use of RDDs DataFrames and Datasets\"></a>\n",
    "### Use of RDDs, DataFrames and Datasets\n",
    "\n",
    "So, where should I use RDDs, Datasets or DataFrames in my application? Let's summarize the characteristics of each data structure. You should use...\n",
    "\n",
    "\n",
    "* RDDs when...\n",
    "\n",
    "    - your data is unstructured, for example, binary (media) streams or text streams\n",
    "    - you want to control your dataset and use low-level transformations and actions\n",
    "    - you are ok to miss optimizations for DataFrames and Datasets for structured and semi-structured data that are available out of the box\n",
    "    - you don’t care about the schema, columnar format and ready to use functional programming constructs\n",
    "\n",
    "\n",
    "* DataFrames when...\n",
    "\n",
    "    - your data is structured (RDBMS input) or semi-structured (json, csv)\n",
    "    - you want to get the best performance gained from SQL’s optimized execution engine\n",
    "    - you need to run hive queries\n",
    "    - you appreciate domain specific language API (.groupBy, .agg, .orderBy)\n",
    "    - you are using R or Python \n",
    "\n",
    "\n",
    "* Datasets when...\n",
    "\n",
    "    - your data is structured or semi-structured\n",
    "    - you appreciate type-safety at a compile time and a strong-typed API\n",
    "    - you need good performance (mostly greater than RDD), but not the best one (usually lower than DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
